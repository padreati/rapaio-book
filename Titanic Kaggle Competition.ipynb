{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting started: Kaggle's Titanic Competition\n",
    "\n",
    "Kaggle is already established as the best place which hosts machine learning competitions. If you do not know it already, then it's time to do it.\n",
    "\n",
    "__[Titanic Competition](https://www.kaggle.com/c/titanic)__ is perhaps the first competition which one should try. Of course, if you are already an experienced data scientist, than you can skip to an advanced competition.\n",
    "\n",
    "The purpose of the competition is to learn if a passenger has survived or not. We illustrate some steps and ideas one can apply to compete in this learning competition using the available tools one can find in rapaio library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load rapaio-bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Get the data\n",
    "\n",
    "The purpose of the competition is to predict which passengers have survived or not. The available data has two parts. The first part consists in a data set which contains what happened with some passengers and some related information like sex, cabin, age, class, etc. This data set contains information regarding their survival. The purpose why this data set contains survival data is because it will be used to train a model which learns how to decide if a passenger survives or not. This is the `train.csv`. The other file is a data set which contains data about another set of passenger, this time without knowing if they survived or not. They contain, however an identification number. This data set is `test.csv` and this is used to make predictions. Those predictions should be similar with the provided `gendermodel.csv`.\n",
    "\n",
    "We also have to take a look of the data description provided on __[contest dedicated page](https://www.kaggle.com/c/titanic/data)__:\n",
    "\n",
    "```\n",
    "VARIABLE DESCRIPTIONS:\n",
    "survival Survival\n",
    "(0 = No; 1 = Yes)\n",
    "pclass Passenger Class\n",
    "(1 = 1st; 2 = 2nd; 3 = 3rd)\n",
    "name Name\n",
    "sex Sex\n",
    "age Age\n",
    "sibsp Number of Siblings/Spouses Aboard\n",
    "parch Number of Parents/Children Aboard\n",
    "ticket Ticket Number\n",
    "fare Passenger Fare\n",
    "cabin Cabin\n",
    "embarked Port of Embarkation\n",
    "(C = Cherbourg; Q = Queenstown; S = Southampton)\n",
    "\n",
    "SPECIAL NOTES:\n",
    "Pclass is a proxy for socio-economic status (SES)\n",
    "1st ~ Upper; 2nd ~ Middle; 3rd ~ Lower\n",
    "\n",
    "Age is in Years; Fractional if Age less than One (1)\n",
    "If the Age is Estimated, it is in the form xx.5\n",
    "\n",
    "With respect to the family relation variables (i.e. sibsp and parch)\n",
    "some relations were ignored. The following are the definitions used\n",
    "for sibsp and parch.\n",
    "\n",
    "Sibling: Brother, Sister, Stepbrother, or Stepsister of Passenger Aboard Titanic\n",
    "Spouse: Husband or Wife of Passenger Aboard Titanic (Mistresses and Fiances Ignored)\n",
    "Parent: Mother or Father of Passenger Aboard Titanic\n",
    "Child: Son, Daughter, Stepson, or Stepdaughter of Passenger Aboard Titanic\n",
    "\n",
    "Other family relatives excluded from this study include cousins,\n",
    "nephews/nieces, aunts/uncles, and in-laws. Some children travelled\n",
    "only with a nanny, therefore parch=0 for them. As well, some\n",
    "travelled with very close friends or neighbors in a village, however,\n",
    "the definitions do not support such relations.\n",
    "```\n",
    "\n",
    "The first step in our adventure is to download those 3 data file in *csv* format. You can do it from __[data section](https://www.kaggle.com/c/titanic/data)__ of the competition. Let's suppose you downloaded somewhere in a local folder. We will name this folder `data` folder, and actually it can have any name you would like.\n",
    "\n",
    "### 1.2 Read train data from csv file\n",
    "\n",
    "Because the data is small we can load the whole data in memory with no problems.\n",
    "\n",
    "Let's see how we can load the data into memory. In rapaio the sets of data are loaded into the form of *frames* (`rapaio.data.Frame`). A frame is basically a tabular data, with columns for each variable (feature) and rows for each instance (in our case for each passenger).\n",
    "\n",
    "\n",
    "A first try of loading the train data set and see what has happened is the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Summary\n",
      "=============\n",
      "* rowCount: 891\n",
      "* complete: 183/891\n",
      "* varCount: 12\n",
      "* varNames: \n",
      "\n",
      " 0. PassengerId : dbl |  4.         Sex : nom |  8.      Ticket : nom | \n",
      " 1.    Survived : bin |  5.         Age : dbl |  9.        Fare : dbl | \n",
      " 2.      Pclass : dbl |  6.       SibSp : dbl | 10.       Cabin : nom | \n",
      " 3.        Name : nom |  7.       Parch : dbl | 11.    Embarked : nom | \n",
      "\n",
      "* summary: \n",
      " PassengerId [dbl]        Survived [bin]    Pclass [dbl]     \n",
      "      Min. :   1.0000000 0 :         549    Min. : 1.0000000 \n",
      "   1st Qu. : 223.5000000 1 :         342 1st Qu. : 2.0000000 \n",
      "    Median : 446.0000000 NAs :         0  Median : 3.0000000 \n",
      "      Mean : 446.0000000                    Mean : 2.3086420 \n",
      "   2nd Qu. : 668.5000000                 2nd Qu. : 3.0000000 \n",
      "      Max. : 891.0000000                    Max. : 3.0000000 \n",
      "                                                             \n",
      "\n",
      "                                                   Name [nom]      Sex [nom] \n",
      "\"Jacobsohn, Mrs. Sidney Samuel (Amy Frances Christy)\" :     1   male :   577 \n",
      "                                \"Toomey, Miss. Ellen\" :     1 female :   314 \n",
      "                          \"de Pelsmaeker, Mr. Alfons\" :     1                \n",
      "                          \"Jensen, Mr. Svend Lauritz\" :     1                \n",
      "                                \"Morley, Mr. William\" :     1                \n",
      "                                              (Other) :   886                \n",
      "                                                                             \n",
      "\n",
      "      Age [dbl]           SibSp [dbl]         Parch [dbl]         Ticket [nom]      Fare \n",
      "   Min. :   0.4200000    Min. : 0.0000000    Min. : 0.0000000 CA. 2343 :     7    Min. : \n",
      "1st Qu. :  20.1250000 1st Qu. : 0.0000000 1st Qu. : 0.0000000   347082 :     7 1st Qu. : \n",
      " Median :  28.0000000  Median : 0.0000000  Median : 0.0000000     1601 :     7  Median : \n",
      "   Mean :  29.6991176    Mean : 0.5230079    Mean : 0.3815937   347088 :     6    Mean : \n",
      "2nd Qu. :  38.0000000 2nd Qu. : 1.0000000 2nd Qu. : 0.0000000  3101295 :     6 2nd Qu. : \n",
      "   Max. :  80.0000000    Max. : 8.0000000    Max. : 6.0000000  (Other) :   858    Max. : \n",
      "    NAs : 177.0000000                                                                    \n",
      "\n",
      "[dbl]               Cabin [nom]  Embarked [nom] \n",
      "  0.0000000 C23 C25 C27 :     4       S :   644 \n",
      "  7.9104000     B96 B98 :     4       C :   168 \n",
      " 14.4542000          G6 :     4       Q :    77 \n",
      " 32.2042080          F2 :     3                 \n",
      " 31.0000000     (Other) :   189                 \n",
      "512.3292000         NAs :   687     NAs :     2 \n",
      "                                                \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "String urlTrain = \"https://raw.githubusercontent.com/padreati/rapaio-notebooks/master/data/titanic/train.csv\";\n",
    "String urlTest = \"https://raw.githubusercontent.com/padreati/rapaio-notebooks/master/data/titanic/test.csv\";\n",
    "Csv.instance().readUrl(urlTrain).printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we interpret the output of the frame's summary?\n",
    "\n",
    "* We loaded a frame which has $891$ rows and $12$ columns (variables)\n",
    "* From all the rows, $183$ are complete (non missing data)\n",
    "* The name of the variables are listed, together with their types\n",
    "* It follows a data summary for the frame: 6 number summary for numeric variables, most frequent levels for nominal variables\n",
    "\n",
    "Let's inspect each variable and see how it fits our needs.\n",
    "\n",
    "**PassengedId**\n",
    "\n",
    "The type for this variable is index (integer values). This field looks like an identifier for the passenger, so from our point of view the sorting is not required. What we can do, but is not required, is to change the field type to nominal. Anyway, we do not need this field for learning since it should be unique for each instance, thus the predictive power is null. We will ignore it for now since we will not consider it for learning\n",
    "\n",
    "**Survived**\n",
    "\n",
    "This is our target variable. It is parsed as binary, but since we do classification, we will change it's type to nominal. We do that directly from the csv parsing, by indicating that we want Survived parsed as nominal variable:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Summary\n",
      "=============\n",
      "* rowCount: 891\n",
      "* complete: 183/891\n",
      "* varCount: 12\n",
      "* varNames: \n",
      "\n",
      " 0. PassengerId : dbl |  4.         Sex : nom |  8.      Ticket : nom | \n",
      " 1.    Survived : nom |  5.         Age : dbl |  9.        Fare : dbl | \n",
      " 2.      Pclass : dbl |  6.       SibSp : dbl | 10.       Cabin : nom | \n",
      " 3.        Name : nom |  7.       Parch : dbl | 11.    Embarked : nom | \n",
      "\n",
      "* summary: \n",
      " PassengerId [dbl]        Survived [nom]    Pclass [dbl]     \n",
      "      Min. :   1.0000000       0 :   549    Min. : 1.0000000 \n",
      "   1st Qu. : 223.5000000       1 :   342 1st Qu. : 2.0000000 \n",
      "    Median : 446.0000000                  Median : 3.0000000 \n",
      "      Mean : 446.0000000                    Mean : 2.3086420 \n",
      "   2nd Qu. : 668.5000000                 2nd Qu. : 3.0000000 \n",
      "      Max. : 891.0000000                    Max. : 3.0000000 \n",
      "                                                             \n",
      "\n",
      "                                                   Name [nom]      Sex [nom] \n",
      "\"Jacobsohn, Mrs. Sidney Samuel (Amy Frances Christy)\" :     1   male :   577 \n",
      "                                \"Toomey, Miss. Ellen\" :     1 female :   314 \n",
      "                          \"de Pelsmaeker, Mr. Alfons\" :     1                \n",
      "                          \"Jensen, Mr. Svend Lauritz\" :     1                \n",
      "                                \"Morley, Mr. William\" :     1                \n",
      "                                              (Other) :   886                \n",
      "                                                                             \n",
      "\n",
      "      Age [dbl]           SibSp [dbl]         Parch [dbl]         Ticket [nom]      Fare \n",
      "   Min. :   0.4200000    Min. : 0.0000000    Min. : 0.0000000 CA. 2343 :     7    Min. : \n",
      "1st Qu. :  20.1250000 1st Qu. : 0.0000000 1st Qu. : 0.0000000   347082 :     7 1st Qu. : \n",
      " Median :  28.0000000  Median : 0.0000000  Median : 0.0000000     1601 :     7  Median : \n",
      "   Mean :  29.6991176    Mean : 0.5230079    Mean : 0.3815937   347088 :     6    Mean : \n",
      "2nd Qu. :  38.0000000 2nd Qu. : 1.0000000 2nd Qu. : 0.0000000  3101295 :     6 2nd Qu. : \n",
      "   Max. :  80.0000000    Max. : 8.0000000    Max. : 6.0000000  (Other) :   858    Max. : \n",
      "    NAs : 177.0000000                                                                    \n",
      "\n",
      "[dbl]               Cabin [nom]  Embarked [nom] \n",
      "  0.0000000 C23 C25 C27 :     4       S :   644 \n",
      "  7.9104000     B96 B98 :     4       C :   168 \n",
      " 14.4542000          G6 :     4       Q :    77 \n",
      " 32.2042080          F2 :     3                 \n",
      " 31.0000000     (Other) :   189                 \n",
      "512.3292000         NAs :   687     NAs :     2 \n",
      "                                                \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Csv.instance()\n",
    ".types.add(VarType.NOMINAL, \"Survived\")\n",
    ".readUrl(urlTrain)\n",
    ".printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And notice how type of the `Survived` variable changed to nominal.\n",
    "\n",
    "\n",
    "**Pclass**\n",
    "\n",
    "This variable has index type. We can keep it like it is or we can change it to nominal. Both ways can be useful. For example parsed as index could give an interpretation to the order. We can say that somehow, because of ordering class 1 is lower than class 2, and class 2 is between classes 1 and 3. At the same time we can keep it as nominal if we do not want to use the ordering. Let's choose nominal for now, considering that 1,2 and 3 are just labels for type of tickets, with no other meaning attached. We proceed in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Summary\n",
      "=============\n",
      "* rowCount: 891\n",
      "* complete: 183/891\n",
      "* varCount: 12\n",
      "* varNames: \n",
      "\n",
      " 0. PassengerId : dbl |  4.         Sex : nom |  8.      Ticket : nom | \n",
      " 1.    Survived : nom |  5.         Age : dbl |  9.        Fare : dbl | \n",
      " 2.      Pclass : nom |  6.       SibSp : dbl | 10.       Cabin : nom | \n",
      " 3.        Name : nom |  7.       Parch : dbl | 11.    Embarked : nom | \n",
      "\n",
      "* summary: \n",
      " PassengerId [dbl]        Survived [nom]  Pclass [nom] \n",
      "      Min. :   1.0000000       0 :   549     3 :   491 \n",
      "   1st Qu. : 223.5000000       1 :   342     1 :   216 \n",
      "    Median : 446.0000000                     2 :   184 \n",
      "      Mean : 446.0000000                               \n",
      "   2nd Qu. : 668.5000000                               \n",
      "      Max. : 891.0000000                               \n",
      "                                                       \n",
      "\n",
      "                                                   Name [nom]      Sex [nom] \n",
      "\"Jacobsohn, Mrs. Sidney Samuel (Amy Frances Christy)\" :     1   male :   577 \n",
      "                                \"Toomey, Miss. Ellen\" :     1 female :   314 \n",
      "                          \"de Pelsmaeker, Mr. Alfons\" :     1                \n",
      "                          \"Jensen, Mr. Svend Lauritz\" :     1                \n",
      "                                \"Morley, Mr. William\" :     1                \n",
      "                                              (Other) :   886                \n",
      "                                                                             \n",
      "\n",
      "      Age [dbl]           SibSp [dbl]         Parch [dbl]         Ticket [nom]      Fare \n",
      "   Min. :   0.4200000    Min. : 0.0000000    Min. : 0.0000000 CA. 2343 :     7    Min. : \n",
      "1st Qu. :  20.1250000 1st Qu. : 0.0000000 1st Qu. : 0.0000000   347082 :     7 1st Qu. : \n",
      " Median :  28.0000000  Median : 0.0000000  Median : 0.0000000     1601 :     7  Median : \n",
      "   Mean :  29.6991176    Mean : 0.5230079    Mean : 0.3815937   347088 :     6    Mean : \n",
      "2nd Qu. :  38.0000000 2nd Qu. : 1.0000000 2nd Qu. : 0.0000000  3101295 :     6 2nd Qu. : \n",
      "   Max. :  80.0000000    Max. : 8.0000000    Max. : 6.0000000  (Other) :   858    Max. : \n",
      "    NAs : 177.0000000                                                                    \n",
      "\n",
      "[dbl]               Cabin [nom]  Embarked [nom] \n",
      "  0.0000000 C23 C25 C27 :     4       S :   644 \n",
      "  7.9104000     B96 B98 :     4       C :   168 \n",
      " 14.4542000          G6 :     4       Q :    77 \n",
      " 32.2042080          F2 :     3                 \n",
      " 31.0000000     (Other) :   189                 \n",
      "512.3292000         NAs :   687     NAs :     2 \n",
      "                                                \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Csv.instance()\n",
    ".types.add(VarType.NOMINAL, \"Survived\", \"Pclass\")\n",
    ".readUrl(urlTrain)\n",
    ".printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we append the variable name after `Survived`. This is possible since the `withTypes` method specify a type, and follows a dynamic array of strings, for the names of variables.\n",
    "\n",
    "**Name**\n",
    "\n",
    "This is the passenger names and the values are unique. As it is, the predictive power of this field is null. We keep it as it is. Note that it contains valuable information, but not in this direct form.\n",
    "\n",
    "**Sex**\n",
    "\n",
    "This field specifies the gender of the passenger. We have $577$ males and $314$ females.\n",
    "\n",
    "**Age**\n",
    "\n",
    "This field specifies the age of an passenger. We would expect that to parse this variable as numeric or at leas index, but is nominal. Why that happened? Notice that the values looks like numbers. But the first value (the most frequent one, $117$ instances) has nothing specified. Well, the variable is nominal has to do with how *Csv* parsing handles missing values. By default, the *csv* parsing considers as missing values only the string \"?\". But the most frequent value in this field is empty string \"\". This means that empty string is not considered a missing value. Because empty string can't produce numbers from parsing, the variable is *promoted* to nominal.\n",
    "\n",
    "We can customize the missing value handling by specifying the valid strings for that purpose. We use `.useNAValues(String...naValues)` to tell the parser all the valid strings which are missing values. In our case we want just the empty string to be a missing value. When the parser will found an empty string it will set the variable value as missing value. It will *not promote* variable to nominal, since a missing value is a legal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Summary\n",
      "=============\n",
      "* rowCount: 891\n",
      "* complete: 183/891\n",
      "* varCount: 12\n",
      "* varNames: \n",
      "\n",
      " 0. PassengerId : dbl |  4.         Sex : nom |  8.      Ticket : nom | \n",
      " 1.    Survived : nom |  5.         Age : dbl |  9.        Fare : dbl | \n",
      " 2.      Pclass : nom |  6.       SibSp : dbl | 10.       Cabin : nom | \n",
      " 3.        Name : nom |  7.       Parch : dbl | 11.    Embarked : nom | \n",
      "\n",
      "* summary: \n",
      " PassengerId [dbl]        Survived [nom]  Pclass [nom] \n",
      "      Min. :   1.0000000       0 :   549     3 :   491 \n",
      "   1st Qu. : 223.5000000       1 :   342     1 :   216 \n",
      "    Median : 446.0000000                     2 :   184 \n",
      "      Mean : 446.0000000                               \n",
      "   2nd Qu. : 668.5000000                               \n",
      "      Max. : 891.0000000                               \n",
      "                                                       \n",
      "\n",
      "                                                   Name [nom]      Sex [nom] \n",
      "\"Jacobsohn, Mrs. Sidney Samuel (Amy Frances Christy)\" :     1   male :   577 \n",
      "                                \"Toomey, Miss. Ellen\" :     1 female :   314 \n",
      "                          \"de Pelsmaeker, Mr. Alfons\" :     1                \n",
      "                          \"Jensen, Mr. Svend Lauritz\" :     1                \n",
      "                                \"Morley, Mr. William\" :     1                \n",
      "                                              (Other) :   886                \n",
      "                                                                             \n",
      "\n",
      "      Age [dbl]           SibSp [dbl]         Parch [dbl]         Ticket [nom]      Fare \n",
      "   Min. :   0.4200000    Min. : 0.0000000    Min. : 0.0000000 CA. 2343 :     7    Min. : \n",
      "1st Qu. :  20.1250000 1st Qu. : 0.0000000 1st Qu. : 0.0000000   347082 :     7 1st Qu. : \n",
      " Median :  28.0000000  Median : 0.0000000  Median : 0.0000000     1601 :     7  Median : \n",
      "   Mean :  29.6991176    Mean : 0.5230079    Mean : 0.3815937   347088 :     6    Mean : \n",
      "2nd Qu. :  38.0000000 2nd Qu. : 1.0000000 2nd Qu. : 0.0000000  3101295 :     6 2nd Qu. : \n",
      "   Max. :  80.0000000    Max. : 8.0000000    Max. : 6.0000000  (Other) :   858    Max. : \n",
      "    NAs : 177.0000000                                                                    \n",
      "\n",
      "[dbl]               Cabin [nom]  Embarked [nom] \n",
      "  0.0000000 C23 C25 C27 :     4       S :   644 \n",
      "  7.9104000     B96 B98 :     4       C :   168 \n",
      " 14.4542000          G6 :     4       Q :    77 \n",
      " 32.2042080          F2 :     3                 \n",
      " 31.0000000     (Other) :   189                 \n",
      "512.3292000         NAs :   687     NAs :     2 \n",
      "                                                \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Csv.instance()\n",
    ".naValues.add(\"\")\n",
    ".types.add(VarType.NOMINAL, \"Survived\", \"Pclass\")\n",
    ".readUrl(urlTrain)\n",
    ".printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice what happened: *Age* field is now numeric and it contains $177$ missing values.\n",
    "\n",
    "**SibSp**\n",
    "\n",
    "It's meaning is \"siblings/spouses\". It's parsed as index, which is natural. In pathological cases with sick imagination we can consider a \"quarter of a wife\" for example.\n",
    "\n",
    "**Parch**\n",
    "\n",
    "It's meaning is \"parents/children\". It is naturally parsed as index.\n",
    "\n",
    "**Ticket**\n",
    "\n",
    "This is the code of the ticket. Probably a family can have the same ticket, thus must be the reason why the frequencies have values up to $$7$$. This field is nominal. It has low predictive power used directly. Perhaps contains valuable information, but used directly in row format would not help much.\n",
    "\n",
    "**Fare**\n",
    "\n",
    "This is the price for passenger fare and should be numeric, like it is.\n",
    "\n",
    "**Cabin**\n",
    "\n",
    "Code of the passenger's cabin, parsed as nominal. Same notes as for `Ticket` variable.\n",
    "\n",
    "**Embarked**\n",
    "\n",
    "Code for the embarking city, which could be: C = Cherbourg, Q = Queenstown, S = Southampton. It's parsed as nominal and has $2$ missing values.\n",
    "\n",
    "If we are content with our parsing, we load data into a data frame for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Summary\n",
      "=============\n",
      "* rowCount: 891\n",
      "* complete: 183/891\n",
      "* varCount: 12\n",
      "* varNames: \n",
      "\n",
      " 0. PassengerId : dbl |  4.         Sex : nom |  8.      Ticket : nom | \n",
      " 1.    Survived : nom |  5.         Age : dbl |  9.        Fare : dbl | \n",
      " 2.      Pclass : nom |  6.       SibSp : dbl | 10.       Cabin : nom | \n",
      " 3.        Name : nom |  7.       Parch : dbl | 11.    Embarked : nom | \n",
      "\n",
      "* summary: \n",
      " PassengerId [dbl]        Survived [nom]  Pclass [nom] \n",
      "      Min. :   1.0000000       0 :   549     3 :   491 \n",
      "   1st Qu. : 223.5000000       1 :   342     1 :   216 \n",
      "    Median : 446.0000000                     2 :   184 \n",
      "      Mean : 446.0000000                               \n",
      "   2nd Qu. : 668.5000000                               \n",
      "      Max. : 891.0000000                               \n",
      "                                                       \n",
      "\n",
      "                                                   Name [nom]      Sex [nom] \n",
      "\"Jacobsohn, Mrs. Sidney Samuel (Amy Frances Christy)\" :     1   male :   577 \n",
      "                                \"Toomey, Miss. Ellen\" :     1 female :   314 \n",
      "                          \"de Pelsmaeker, Mr. Alfons\" :     1                \n",
      "                          \"Jensen, Mr. Svend Lauritz\" :     1                \n",
      "                                \"Morley, Mr. William\" :     1                \n",
      "                                              (Other) :   886                \n",
      "                                                                             \n",
      "\n",
      "      Age [dbl]           SibSp [dbl]         Parch [dbl]         Ticket [nom]      Fare \n",
      "   Min. :   0.4200000    Min. : 0.0000000    Min. : 0.0000000 CA. 2343 :     7    Min. : \n",
      "1st Qu. :  20.1250000 1st Qu. : 0.0000000 1st Qu. : 0.0000000   347082 :     7 1st Qu. : \n",
      " Median :  28.0000000  Median : 0.0000000  Median : 0.0000000     1601 :     7  Median : \n",
      "   Mean :  29.6991176    Mean : 0.5230079    Mean : 0.3815937   347088 :     6    Mean : \n",
      "2nd Qu. :  38.0000000 2nd Qu. : 1.0000000 2nd Qu. : 0.0000000  3101295 :     6 2nd Qu. : \n",
      "   Max. :  80.0000000    Max. : 8.0000000    Max. : 6.0000000  (Other) :   858    Max. : \n",
      "    NAs : 177.0000000                                                                    \n",
      "\n",
      "[dbl]               Cabin [nom]  Embarked [nom] \n",
      "  0.0000000 C23 C25 C27 :     4       S :   644 \n",
      "  7.9104000     B96 B98 :     4       C :   168 \n",
      " 14.4542000          G6 :     4       Q :    77 \n",
      " 32.2042080          F2 :     3                 \n",
      " 31.0000000     (Other) :   189                 \n",
      "512.3292000         NAs :   687     NAs :     2 \n",
      "                                                \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Frame train = Csv.instance().naValues.add(\"\").types.add(VarType.NOMINAL, \"Survived\", \"Pclass\").readUrl(urlTrain);\n",
    "train.printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Read test data from *csv* file\n",
    "\n",
    "Once we have a training frame we can load also the test data. We do that to take a look at the frame and because data is small and there is no memory or time problem cost associated with it. To avoid adding again the *csv* options and to get identical levels nominal variables, we use a different way to parse the data set. We specify variable types by frame templates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Frame test = Csv.instance().naValues.add(\"\").template.set(train).readUrl(urlTest);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead to specify again the preferred types for variables, we use train frame as a template for variable types. This has also the side effect that the encoding of categorical variables is identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Summary\n",
      "=============\n",
      "* rowCount: 418\n",
      "* complete: 87/418\n",
      "* varCount: 11\n",
      "* varNames: \n",
      "\n",
      " 0. PassengerId : dbl |  4.         Age : dbl |  8.        Fare : dbl | \n",
      " 1.      Pclass : nom |  5.       SibSp : dbl |  9.       Cabin : nom | \n",
      " 2.        Name : nom |  6.       Parch : dbl | 10.    Embarked : nom | \n",
      " 3.         Sex : nom |  7.      Ticket : nom | \n",
      "\n",
      "* summary: \n",
      " PassengerId [dbl]          Pclass [nom]                                   Name [nom] \n",
      "      Min. :   892.0000000     3 :   218                \"Birnbaum, Mr. Jakob\" :     1 \n",
      "   1st Qu. :   996.2500000     1 :   107 \"Willer, Mr. Aaron (Abi Weller\"\")\"\"\" :     1 \n",
      "    Median : 1,100.5000000     2 :    93          \"Olsson, Mr. Oscar Wilhelm\" :     1 \n",
      "      Mean : 1,100.5000000                               \"Barry, Miss. Julia\" :     1 \n",
      "   2nd Qu. : 1,204.7500000                                 \"Thomas, Mr. John\" :     1 \n",
      "      Max. : 1,309.0000000                                            (Other) :   413 \n",
      "                                                                                      \n",
      "\n",
      "     Sex [nom]       Age [dbl]          SibSp [dbl]         Parch [dbl]         Ticket \n",
      "  male :   266    Min. :  0.1700000    Min. : 0.0000000    Min. : 0.0000000 PC 17608 : \n",
      "female :   152 1st Qu. : 21.0000000 1st Qu. : 0.0000000 1st Qu. : 0.0000000 CA. 2343 : \n",
      "                Median : 27.0000000  Median : 0.0000000  Median : 0.0000000   113503 : \n",
      "                  Mean : 30.2725904    Mean : 0.4473684    Mean : 0.3923445    16966 : \n",
      "               2nd Qu. : 39.0000000 2nd Qu. : 1.0000000 2nd Qu. : 0.0000000   347077 : \n",
      "                  Max. : 76.0000000    Max. : 8.0000000    Max. : 9.0000000  (Other) : \n",
      "                   NAs : 86.0000000                                                    \n",
      "\n",
      "[nom]      Fare [dbl]                   Cabin [nom]  Embarked [nom] \n",
      "    5    Min. :   0.0000000 B57 B59 B63 B66 :     3       S :   270 \n",
      "    4 1st Qu. :   7.8958000            C116 :     2       C :   102 \n",
      "    4  Median :  14.4542000            C101 :     2       Q :    46 \n",
      "    3    Mean :  35.6271885              F4 :     2                 \n",
      "    3 2nd Qu. :  31.5000000         (Other) :    82                 \n",
      "  399    Max. : 512.3292000             NAs :   327                 \n",
      "          NAs :   1.0000000                                         \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can note that we don't have *Survived* variable anymore. This is correct since this is what we have to predict. Note also that the types for the remaining variables are the same with training data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build simple models\n",
    "\n",
    "### 2.1 Build a majority model\n",
    "\n",
    "To make a first submission we will build a very simple model, which classifies with a single value all instances. This value is the majority label.\n",
    "\n",
    "Let's inspect at how target variable look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0   1 \n",
      "  -   - \n",
      "549 342 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "DensityVector.fromLevelCounts(false, train.rvar(\"Survived\")).printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we already new from the summary, the number of passengers who didn't survived is lower than those who did. Let's see percentages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0         1 \n",
      "        -         - \n",
      "0.6161616 0.3838384 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "DensityVector.fromLevelCounts(false, train.rvar(\"Survived\")).normalize().printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar result can be obtined using group functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group by: Survived\n",
      "group count: 2\n",
      "group by functions: GroupByFunction{name=count,varNames=[Survived]}\n",
      "\n",
      "    Survived Survived_count_N1  \n",
      "[0]        0 0.6161616161616161 \n",
      "[1]        1 0.3838383838383838 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Group.from(train, \"Survived\").aggregate(Group.count(1, \"Survived\")).printContent();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that there are about $61\\%$ of passengers who did not survived. We will create a submit data set, which we will save for later submission. How we can do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "VarNominal prediction = VarNominal.from(test.rowCount(), row -> \"0\").name(\"Survived\");\n",
    "Frame submit = SolidFrame.byVars(test.rvar(\"PassengerId\"), prediction);\n",
    "\n",
    "Csv.instance().quotes.set(false).write(submit, \"majority_submit.csv\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first line we created a new nominal variable. The size of the new variable is the number of rows from the test frame. For each row we produce the same label `\"0\"`. We name this variable `Survived`.\n",
    "\n",
    "In the second line we created a new frame taking the variable named `PassengerId` from the test data set and the new prediction variable.\n",
    "\n",
    "In the last line we wrote a new csv file with the csv parsing utility, taking care to not write quotes. We can submit this file and see which are the results.\n",
    "\n",
    "![Submission result with majority classifierModel](images/titanic-majority-submit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Build a simple gender model\n",
    "\n",
    "It has been said that \"women and children first\" really happened during Titanic tragedy. If this was true or not, we do not know. But we can use data to see if we are hearing the same story. For now we will take the gender and see if it had an influence. We will build a contingency table for variables `Sex` and `Survived`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0   1 total \n",
      "  male 468 109  577  \n",
      "female  81 233  314  \n",
      " total 549 342  891  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "DensityTable.fromLevelCounts(false, train.rvar(\"Sex\"), train.rvar(\"Survived\")).printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On rows we have levels of `Sex` variable. On columns we have levels of `Sex` variable. Cells are computed as counts. What we see is that there are a lot of men who did not survived and a lot of women who does. We will normalize on rows to take a closer look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               0         1 total \n",
      "  male 0.8110919 0.1889081   1   \n",
      "female 0.2579618 0.7420382   1   \n",
      " total 1.0690536 0.9309464   2   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "DensityTable.fromLevelCounts(false, train.rvar(\"Sex\"), train.rvar(\"Survived\")).normalizeOnRows().printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that men survived with a rate of $0.19$ and women with $0.74$. The values are so obvious, we need no hypothesis testing to check that this variable is significant for classification. We will build a simple model where we predict as survived all the women and not survived all the men."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Var prediction = VarNominal.from(test.rowCount(), row -> test.getLabel(row, \"Sex\").equals(\"male\") ? \"0\" : \"1\").name(\"Survived\");\n",
    "Frame submit = SolidFrame.byVars(test.rvar(\"PassengerId\"), prediction);\n",
    "Csv.instance().write(submit, \"gender_submit.csv\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__![Submission result with gender classifierModel](images/titanic-gender-submit.png)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tree model\n",
    "\n",
    "Building models in the manual way is often not the way to go. This process is tedious and time consuming. There are already built automated procedures, which incorporate miscellaneous approaches to learn a classifierModel. One of the often used models is the decision tree. Decision trees are greedy local approximations build in a recursive greedy fashion. Often the split decision at node level uses a single feature. At leave nodes a simple majority classifierModel creates the classification rule.\n",
    "\n",
    "### 3.1 Gender model with decision tree\n",
    "Initially we will build a CART decision tree using as input feature the *Sex* variable. We do this to exemplify how a manual rule can be created in an automated fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTree model\n",
      "================\n",
      "\n",
      "Description:\n",
      "CTree{purity=GiniGain,splitter=Random,varSelector=VarSelector[ALL]}\n",
      "\n",
      "Capabilities:\n",
      "types inputs/targets: NOMINAL,INT,DOUBLE,BINARY/NOMINAL\n",
      "counts inputs/targets: [1,1000000] / [1,1]\n",
      "missing inputs/targets: true/false\n",
      "\n",
      "Learned model:\n",
      "\n",
      "total number of nodes: 3\n",
      "total number of leaves: 2\n",
      "description:\n",
      "split, n/err, classes (densities) [* if is leaf / purity if not]\n",
      "\n",
      "|- 0. root    891/342 0 (0.616 0.384 ) [0.139648]\n",
      "|   |- 1. Sex='male'    577/109 0 (0.811 0.189 ) *\n",
      "|   |- 2. Sex!='male'    314/81 1 (0.258 0.742 ) *\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Frame tr = train.mapVars(\"Survived,Sex\");\n",
    "CTree tree = CTree.newCART();\n",
    "tree.fit(tr, \"Survived\");\n",
    "tree.printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a closer look at the last three rows from the output, one can identify our manual rule. Basically the interpretation is: *\"all the females survived, all the males did not\"*. For exemplification purposes we build also the submit file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\n",
      "CTree{purity=GiniGain,splitter=Random,varSelector=VarSelector[ALL]}\n",
      "CV score in training data\n",
      "=========================\n",
      "    dataset  metric    mean       std    \n",
      "[0]    test Accuracy 0.786779  0.0463445 \n",
      "[1]   train Accuracy 0.7867567 0.0051443 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// do a cross validation\n",
    "ClassifierEvaluation.cv(tr, \"Survived\", tree, 10, Accuracy.newMetric(true)).run().printContent();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "// fit the tree to test data frame\n",
    "ClassifierResult pred = tree.predict(test);\n",
    "// build teh submission\n",
    "Frame submit = SolidFrame.byVars(test.rvar(\"PassengerId\"),pred.firstClasses().name(\"Survived\"));\n",
    "// write to a submit file\n",
    "Csv.instance().write(submit, \"tree1-model.csv\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Enrich tree by using other features\n",
    "\n",
    "Our training data set has more than a single input feature. Thus We can state we didn't use all the information available. We will add now the class and embarking port and see how it behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTree model\n",
      "================\n",
      "\n",
      "Description:\n",
      "CTree{purity=GiniGain,splitter=Random,varSelector=VarSelector[ALL]}\n",
      "\n",
      "Capabilities:\n",
      "types inputs/targets: NOMINAL,INT,DOUBLE,BINARY/NOMINAL\n",
      "counts inputs/targets: [1,1000000] / [1,1]\n",
      "missing inputs/targets: true/false\n",
      "\n",
      "Learned model:\n",
      "\n",
      "total number of nodes: 11\n",
      "total number of leaves: 6\n",
      "description:\n",
      "split, n/err, classes (densities) [* if is leaf / purity if not]\n",
      "\n",
      "|- 0. root    891/342 0 (0.616 0.384 ) [0.0000008]\n",
      "|   |- 1. Embarked='C'    170/75 1 (0.441 0.559 ) [0.0000175]\n",
      "|   |   |- 3. Pclass='1'    87/26 1 (0.299 0.701 ) [0.1639784]\n",
      "|   |   |   |- 7. Sex='male'    42/17 0 (0.595 0.405 ) *\n",
      "|   |   |   |- 8. Sex!='male'    45/1 1 (0.022 0.978 ) *\n",
      "|   |   |- 4. Pclass!='1'    83/34 0 (0.59 0.41 ) [0]\n",
      "|   |   |   |- 9. Sex='female'    30/8 1 (0.267 0.733 ) *\n",
      "|   |   |   |- 10. Sex!='female'    53/12 0 (0.774 0.226 ) *\n",
      "|   |- 2. Embarked!='C'    721/247 0 (0.657 0.343 ) [0.1258008]\n",
      "|   |   |- 5. Sex='male'    482/80 0 (0.834 0.166 ) *\n",
      "|   |   |- 6. Sex!='male'    239/72 1 (0.301 0.699 ) *\n",
      "\n",
      "Model:\n",
      "CTree{purity=GiniGain,splitter=Random,varSelector=VarSelector[ALL]}\n",
      "CV score in training data\n",
      "=========================\n",
      "    dataset  metric    mean       std    \n",
      "[0]    test Accuracy 0.7867041 0.0549629 \n",
      "[1]   train Accuracy 0.7866311 0.0060893 \n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "CompilationException",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30mFrame submit = SolidFrame.byVars(test.rvar(\"PassengerId\"),\u001b[0m\u001b[1m\u001b[30m\u001b[41mpred.firstClasses().withName\u001b[0m\u001b[1m\u001b[30m(\"Survived\"));\u001b[0m",
      "\u001b[1m\u001b[31mcannot find symbol\u001b[0m",
      "\u001b[1m\u001b[31m  symbol:   method withName(java.lang.String)\u001b[0m",
      ""
     ]
    }
   ],
   "source": [
    "Frame tr = train.mapVars(\"Survived,Sex,Pclass,Embarked\");\n",
    "\n",
    "CTree tree = CTree.newCART();\n",
    "tree.fit(tr, \"Survived\");\n",
    "tree.printSummary();\n",
    "\n",
    "ClassifierEvaluation.cv(tr, \"Survived\", tree, 10, Accuracy.newMetric(true)).run().printContent();\n",
    "\n",
    "ClassifierResult pred = tree.predict(test);\n",
    "Frame submit = SolidFrame.byVars(test.rvar(\"PassengerId\"),pred.firstClasses().withName(\"Survived\"));\n",
    "Csv.instance().withQuotes(false).write(submit, \"tree2-model.csv\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tree is much richer and there are more chances to be better. This is what happened after submission.\n",
    "\n",
    "__![Results after submission of enriched tree](images/titanic-tree2-submit.png)__\n",
    "\n",
    "**Nice!**. We advanced $704$ positions and improved our score with $0.01435$. On public leader board we have a nice $0.77990$ accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Overfitting with trees\n",
    "\n",
    "What about using other input features to improve our prediction accuracy? There are some of them which we can include directly, with no changes: *Age*,*Fare*,*SibSp* and *Parch*.\n",
    "\n",
    "We can change the script slightly, to include those new input features. But we can do better, we can use cross-validation to estimate what will happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\n",
      "CTree{purity=GiniGain,splitter=Random,varSelector=VarSelector[ALL]}\n",
      "CV score in training data\n",
      "=========================\n",
      "    dataset  metric    mean       std    \n",
      "[0]    test Accuracy 0.7946192 0.0236414 \n",
      "[1]   train Accuracy 0.7875046 0.0045662 \n",
      "\n",
      "\n",
      "Model:\n",
      "CTree{purity=GiniGain,splitter=Random,varSelector=VarSelector[ALL]}\n",
      "CV score in training data\n",
      "=========================\n",
      "    dataset  metric    mean       std    \n",
      "[0]    test Accuracy 0.7979775 0.0494093 \n",
      "[1]   train Accuracy 0.898865  0.0210912 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CTree tree = CTree.newCART();\n",
    "ClassifierEvaluation.cv(train.mapVars(\"Survived,Sex,Pclass,Embarked\"),\"Survived\", tree, 10, Accuracy.newMetric(true)).run().printContent();\n",
    "ClassifierEvaluation.cv(train.mapVars(\"Survived,Sex,Pclass,Embarked,Age,Fare,SibSp,Parch\"),\"Survived\", tree, 10, Accuracy.newMetric(true)).run().printContent();;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the 10-crossfold estimator of the accuracy has dropped with a large quantity. What happens? We can have an idea if we take a look at the learned tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTree model\n",
      "================\n",
      "\n",
      "Description:\n",
      "CTree{purity=GiniGain,splitter=Random,varSelector=VarSelector[ALL]}\n",
      "\n",
      "Capabilities:\n",
      "types inputs/targets: NOMINAL,INT,DOUBLE,BINARY/NOMINAL\n",
      "counts inputs/targets: [1,1000000] / [1,1]\n",
      "missing inputs/targets: true/false\n",
      "\n",
      "Learned model:\n",
      "\n",
      "total number of nodes: 367\n",
      "total number of leaves: 184\n",
      "description:\n",
      "split, n/err, classes (densities) [* if is leaf / purity if not]\n",
      "\n",
      "|- 0. root    891/342 0 (0.616 0.384 ) [0.0416498]\n",
      "|   |- 1. Fare<=10.48125    339/67 0 (0.802 0.198 ) [0.0114513]\n",
      "|   |   |- 3. Age<=32.5    231/54 0 (0.766 0.234 ) [0.0657464]\n",
      "|   |   |   |- 7. Sex='male'    178/24 0 (0.865 0.135 ) [0.010315]\n",
      "|   |   |   |   |- 15. Age<=26.5    117/11 0 (0.906 0.094 ) [0.0027241]\n",
      "|   |   |   |   |   |- 31. Fare<=8.5896    101/11 0 (0.891 0.109 ) [0.0061798]\n",
      "|   |   |   |   |   |   |- 61. Fare<=8.4875    100/10 0 (0.9 0.1 ) [0.0019512]\n",
      "|   |   |   |   |   |   |   |- 101. Fare<=7.2396    24/4 0 (0.833 0.167 ) [0.0190365]\n",
      "|   |   |   |   |   |   |   |   |- 133. Fare<=7.13335    15/1 0 (0.933 0.067 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |- 163. Fare<=2.00625    4/1 0 (0.75 0.25 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 164. Fare>2.00625    11/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |- 134. Fare>7.13335    9/3 0 (0.667 0.333 ) [0.1111111]\n",
      "|   |   |   |   |   |   |   |   |   |- 165. Fare<=7.2271    3/1 1 (0.333 0.667 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 193. Fare<=7.18335    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 194. Fare>7.18335    2/1 1 (0.5 0.5 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 166. Fare>7.2271    6/1 0 (0.833 0.167 ) *\n",
      "|   |   |   |   |   |   |   |- 102. Fare>7.2396    76/6 0 (0.921 0.079 ) [0.0044519]\n",
      "|   |   |   |   |   |   |   |   |- 135. Fare<=7.9104    52/2 0 (0.962 0.038 ) [0.0018491]\n",
      "|   |   |   |   |   |   |   |   |   |- 167. Fare<=7.7979    31/2 0 (0.935 0.065 ) [0.0101085]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 195. Fare<=7.7625    20/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 196. Fare>7.7625    11/2 0 (0.818 0.182 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 231. Fare<=7.7854    7/1 0 (0.857 0.143 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 232. Fare>7.7854    4/1 0 (0.75 0.25 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 168. Fare>7.7979    21/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |- 136. Fare>7.9104    24/4 0 (0.833 0.167 ) [0.0027778]\n",
      "|   |   |   |   |   |   |   |   |   |- 169. Fare<=8.10415    20/4 0 (0.8 0.2 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 197. Fare<=7.9875    4/1 0 (0.75 0.25 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 198. Fare>7.9875    16/3 0 (0.812 0.188 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 170. Fare>8.10415    4/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |- 62. Fare>8.4875    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |- 32. Fare>8.5896    16/0 0 (1 0 ) *\n",
      "|   |   |   |   |- 16. Age>26.5    61/13 0 (0.787 0.213 ) [0.0100758]\n",
      "|   |   |   |   |   |- 33. Fare<=8.08125    52/9 0 (0.827 0.173 ) [0]\n",
      "|   |   |   |   |   |   |- 63. Fare<=3.4875    3/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |- 64. Fare>3.4875    49/9 0 (0.816 0.184 ) [0]\n",
      "|   |   |   |   |   |   |   |- 103. Fare<=7.0104    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |- 104. Fare>7.0104    48/8 0 (0.833 0.167 ) [0.0037037]\n",
      "|   |   |   |   |   |   |   |   |- 137. Fare<=7.7854    19/2 0 (0.895 0.105 ) [0.0041551]\n",
      "|   |   |   |   |   |   |   |   |   |- 171. Fare<=7.7625    16/2 0 (0.875 0.125 ) [0.0020833]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 199. Fare<=7.2271    3/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 200. Fare>7.2271    13/2 0 (0.846 0.154 ) [0.0026627]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 233. Fare<=7.74375    8/1 0 (0.875 0.125 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 269. Fare<=7.2396    5/1 0 (0.8 0.2 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 270. Fare>7.2396    3/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 234. Fare>7.74375    5/1 0 (0.8 0.2 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 172. Fare>7.7625    3/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |- 138. Fare>7.7854    29/6 0 (0.793 0.207 ) [0.0127274]\n",
      "|   |   |   |   |   |   |   |   |   |- 173. Fare<=7.8646    4/2 0 (0.5 0.5 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 201. Fare<=7.825    2/1 0 (0.5 0.5 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 202. Fare>7.825    2/1 1 (0.5 0.5 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 174. Fare>7.8646    25/4 0 (0.84 0.16 ) [0.0021333]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 203. Fare<=7.9104    11/1 0 (0.909 0.091 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 235. Fare<=7.8854    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 236. Fare>7.8854    10/1 0 (0.9 0.1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 204. Fare>7.9104    14/3 0 (0.786 0.214 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 237. Fare<=7.9875    5/2 0 (0.6 0.4 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 238. Fare>7.9875    9/1 0 (0.889 0.111 ) *\n",
      "|   |   |   |   |   |- 34. Fare>8.08125    9/4 0 (0.556 0.444 ) [0]\n",
      "|   |   |   |   |   |   |- 65. Fare<=8.2375    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |- 66. Fare>8.2375    8/3 0 (0.625 0.375 ) [0]\n",
      "|   |   |   |   |   |   |   |- 105. Fare<=8.5125    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |- 106. Fare>8.5125    7/3 0 (0.571 0.429 ) [0]\n",
      "|   |   |   |   |   |   |   |   |- 139. Fare<=8.6875    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |- 140. Fare>8.6875    6/2 0 (0.667 0.333 ) [0.0444444]\n",
      "|   |   |   |   |   |   |   |   |   |- 175. Fare<=9.49165    2/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 176. Fare>9.49165    4/2 1 (0.5 0.5 ) *\n",
      "|   |   |   |- 8. Sex!='male'    53/23 1 (0.434 0.566 ) [0.0710736]\n",
      "|   |   |   |   |- 17. Fare<=8.0396    39/12 1 (0.308 0.692 ) [0.0655329]\n",
      "|   |   |   |   |   |- 35. Age<=27.5    32/8 1 (0.25 0.75 ) [0]\n",
      "|   |   |   |   |   |   |- 67. Fare<=6.9875    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |- 68. Fare>6.9875    31/7 1 (0.226 0.774 ) [0.0244745]\n",
      "|   |   |   |   |   |   |   |- 107. Fare<=7.7625    17/2 1 (0.118 0.882 ) [0.011534]\n",
      "|   |   |   |   |   |   |   |   |- 141. Fare<=7.5229    6/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |- 142. Fare>7.5229    11/2 1 (0.182 0.818 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |- 177. Fare<=7.6    2/1 1 (0.5 0.5 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 178. Fare>7.6    9/1 1 (0.111 0.889 ) [0.0070547]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 205. Fare<=7.74375    3/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 206. Fare>7.74375    6/1 1 (0.167 0.833 ) *\n",
      "|   |   |   |   |   |   |   |- 108. Fare>7.7625    14/5 1 (0.357 0.643 ) [0]\n",
      "|   |   |   |   |   |   |   |   |- 143. Fare<=7.8146    3/1 0 (0.667 0.333 ) *\n",
      "|   |   |   |   |   |   |   |   |- 144. Fare>7.8146    11/3 1 (0.273 0.727 ) [0.0088154]\n",
      "|   |   |   |   |   |   |   |   |   |- 179. Fare<=7.9771    10/3 1 (0.3 0.7 ) [0.0009524]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 207. Fare<=7.9021    5/1 1 (0.2 0.8 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 239. Fare<=7.8667    3/1 1 (0.333 0.667 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 240. Fare>7.8667    2/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 208. Fare>7.9021    5/2 1 (0.4 0.6 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 180. Fare>7.9771    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |- 36. Age>27.5    7/3 0 (0.571 0.429 ) [0.0612245]\n",
      "|   |   |   |   |   |   |- 69. Fare<=7.82085    5/2 1 (0.4 0.6 ) [0]\n",
      "|   |   |   |   |   |   |   |- 109. Fare<=7.6896    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |- 110. Fare>7.6896    4/1 1 (0.25 0.75 ) [0]\n",
      "|   |   |   |   |   |   |   |   |- 145. Fare<=7.76875    3/1 1 (0.333 0.667 ) *\n",
      "|   |   |   |   |   |   |   |   |- 146. Fare>7.76875    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |- 70. Fare>7.82085    2/0 0 (1 0 ) *\n",
      "|   |   |   |   |- 18. Fare>8.0396    14/3 0 (0.786 0.214 ) [0.0250464]\n",
      "|   |   |   |   |   |- 37. Fare<=8.6729    5/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |- 38. Fare>8.6729    9/3 0 (0.667 0.333 ) [0.1111111]\n",
      "|   |   |   |   |   |   |- 71. Fare<=9.5875    3/1 1 (0.333 0.667 ) [0]\n",
      "|   |   |   |   |   |   |   |- 111. Fare<=8.76665    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |- 112. Fare>8.76665    2/1 1 (0.5 0.5 ) [0]\n",
      "|   |   |   |   |   |   |   |   |- 147. Fare<=9.1    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |- 148. Fare>9.1    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |- 72. Fare>9.5875    6/1 0 (0.833 0.167 ) [0.0277778]\n",
      "|   |   |   |   |   |   |   |- 113. Fare<=9.8396    3/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |- 114. Fare>9.8396    3/1 0 (0.667 0.333 ) [0]\n",
      "|   |   |   |   |   |   |   |   |- 149. Fare<=10.1521    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |- 150. Fare>10.1521    2/0 0 (1 0 ) *\n",
      "|   |   |- 4. Age>32.5    108/13 0 (0.88 0.12 ) [0.0670633]\n",
      "|   |   |   |- 9. Sex='male'    95/5 0 (0.947 0.053 ) [0.0032318]\n",
      "|   |   |   |   |- 19. Fare<=7.74165    36/0 0 (1 0 ) *\n",
      "|   |   |   |   |- 20. Fare>7.74165    59/5 0 (0.915 0.085 ) [0.0086827]\n",
      "|   |   |   |   |   |- 39. Age<=38.5    29/1 0 (0.966 0.034 ) [0.0004954]\n",
      "|   |   |   |   |   |   |- 73. Fare<=7.8021    7/1 0 (0.857 0.143 ) [0]\n",
      "|   |   |   |   |   |   |   |- 115. Fare<=7.7625    5/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |- 116. Fare>7.7625    2/1 0 (0.5 0.5 ) *\n",
      "|   |   |   |   |   |   |- 74. Fare>7.8021    22/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |- 40. Age>38.5    30/4 0 (0.867 0.133 ) [0.0048668]\n",
      "|   |   |   |   |   |   |- 75. Fare<=7.9875    16/3 0 (0.812 0.188 ) [0.0078125]\n",
      "|   |   |   |   |   |   |   |- 117. Fare<=7.9104    13/1 0 (0.923 0.077 ) [0]\n",
      "|   |   |   |   |   |   |   |   |- 151. Fare<=7.7625    6/1 0 (0.833 0.167 ) *\n",
      "|   |   |   |   |   |   |   |   |- 152. Fare>7.7625    7/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |- 118. Fare>7.9104    3/1 1 (0.333 0.667 ) *\n",
      "|   |   |   |   |   |   |- 76. Fare>7.9875    14/1 0 (0.929 0.071 ) [0]\n",
      "|   |   |   |   |   |   |   |- 119. Fare<=8.2271    10/1 0 (0.9 0.1 ) *\n",
      "|   |   |   |   |   |   |   |- 120. Fare>8.2271    4/0 0 (1 0 ) *\n",
      "|   |   |   |- 10. Sex!='male'    13/5 1 (0.385 0.615 ) [0.125]\n",
      "|   |   |   |   |- 21. Age<=54    6/2 0 (0.667 0.333 ) [0]\n",
      "|   |   |   |   |   |- 41. Age<=42.5    2/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |- 42. Age>42.5    4/2 0 (0.5 0.5 ) [0]\n",
      "|   |   |   |   |   |   |- 77. Fare<=7.7896    3/1 0 (0.667 0.333 ) *\n",
      "|   |   |   |   |   |   |- 78. Fare>7.7896    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |- 22. Age>54    7/1 1 (0.143 0.857 ) [0.0068027]\n",
      "|   |   |   |   |   |- 43. Fare<=7.8146    4/1 1 (0.25 0.75 ) [0]\n",
      "|   |   |   |   |   |   |- 79. Fare<=7.74165    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |- 80. Fare>7.74165    3/1 1 (0.333 0.667 ) *\n",
      "|   |   |   |   |   |- 44. Fare>7.8146    3/0 1 (0 1 ) *\n",
      "|   |- 2. Fare>10.48125    552/275 0 (0.502 0.498 ) [0.1354255]\n",
      "|   |   |- 5. Sex='male'    304/80 0 (0.737 0.263 ) [0.0275878]\n",
      "|   |   |   |- 11. Age<=6.5    45/20 0 (0.556 0.444 ) [0.0686135]\n",
      "|   |   |   |   |- 23. SibSp<=2.5    34/15 1 (0.441 0.559 ) [0.0243296]\n",
      "|   |   |   |   |   |- 45. Fare<=69.17705    32/15 1 (0.469 0.531 ) [0.0337612]\n",
      "|   |   |   |   |   |   |- 81. Fare<=39.3    29/12 1 (0.414 0.586 ) [0.0253666]\n",
      "|   |   |   |   |   |   |   |- 121. Fare<=36.0021    27/12 1 (0.444 0.556 ) [0.0316049]\n",
      "|   |   |   |   |   |   |   |   |- 153. Fare<=30.75    25/10 1 (0.4 0.6 ) [0.0278261]\n",
      "|   |   |   |   |   |   |   |   |   |- 181. Fare<=25.075    18/9 0 (0.5 0.5 ) [0.0625]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 209. Fare<=21.1271    16/7 1 (0.438 0.562 ) [0.0255208]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 241. Fare<=13.4875    2/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 242. Fare>13.4875    14/7 1 (0.5 0.5 ) [0.05]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 271. Fare<=17.425    10/4 0 (0.6 0.4 ) [0.03]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 297. Fare<=16    9/4 0 (0.556 0.444 ) [0.0604938]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 319. Fare<=15.7    8/3 0 (0.625 0.375 ) [0.03125]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 339. Fare<=15.3729    6/3 0 (0.5 0.5 ) [0.0555556]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 355. Fare<=15.1479    4/1 0 (0.75 0.25 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 365. Fare<=14.775    3/1 0 (0.667 0.333 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 366. Fare>14.775    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 356. Fare>15.1479    2/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 340. Fare>15.3729    2/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 320. Fare>15.7    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 298. Fare>16    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 272. Fare>17.425    4/1 1 (0.25 0.75 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 299. Fare<=19.35835    2/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 300. Fare>19.35835    2/1 1 (0.5 0.5 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 321. Fare<=20.27085    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 322. Fare>20.27085    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 210. Fare>21.1271    2/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 182. Fare>25.075    7/1 1 (0.143 0.857 ) [0.0306122]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 211. Fare<=28.3604    4/1 1 (0.25 0.75 ) [0.125]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 243. Fare<=27.1354    3/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 244. Fare>27.1354    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 212. Fare>28.3604    3/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |- 154. Fare>30.75    2/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |- 122. Fare>36.0021    2/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |- 82. Fare>39.3    3/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |- 46. Fare>69.17705    2/0 1 (0 1 ) *\n",
      "|   |   |   |   |- 24. SibSp>2.5    11/1 0 (0.909 0.091 ) [0.0137741]\n",
      "|   |   |   |   |   |- 47. Fare<=31.33125    6/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |- 48. Fare>31.33125    5/1 0 (0.8 0.2 ) [0]\n",
      "|   |   |   |   |   |   |- 83. Fare<=35.5375    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |- 84. Fare>35.5375    4/0 0 (1 0 ) *\n",
      "|   |   |   |- 12. Age>6.5    259/60 0 (0.768 0.232 ) [0.0213783]\n",
      "|   |   |   |   |- 25. Fare<=26.26875    120/14 0 (0.883 0.117 ) [0.0018946]\n",
      "|   |   |   |   |   |- 49. Fare<=23.35    95/13 0 (0.863 0.137 ) [0.0056724]\n",
      "|   |   |   |   |   |   |- 85. Fare<=22.8875    94/12 0 (0.872 0.128 ) [0.0013861]\n",
      "|   |   |   |   |   |   |   |- 123. Fare<=20.55    89/12 0 (0.865 0.135 ) [0.0038868]\n",
      "|   |   |   |   |   |   |   |   |- 155. Fare<=20.3875    87/11 0 (0.874 0.126 ) [0.0012159]\n",
      "|   |   |   |   |   |   |   |   |   |- 183. Fare<=15.64585    71/8 0 (0.887 0.113 ) [0.0025624]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 213. Fare<=13.93125    57/8 0 (0.86 0.14 ) [0.0018148]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 245. Fare<=13.68125    56/7 0 (0.875 0.125 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 273. Fare<=11.37085    17/3 0 (0.824 0.176 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 301. Fare<=10.87085    16/2 0 (0.875 0.125 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 302. Fare>10.87085    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 274. Fare>11.37085    39/4 0 (0.897 0.103 ) [0.0046022]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 303. Fare<=12.9375    8/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 304. Fare>12.9375    31/4 0 (0.871 0.129 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 323. Fare<=13.25    28/4 0 (0.857 0.143 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 324. Fare>13.25    3/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 246. Fare>13.68125    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 214. Fare>13.93125    14/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 184. Fare>15.64585    16/3 0 (0.812 0.188 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 215. Fare<=15.79585    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 216. Fare>15.79585    15/2 0 (0.867 0.133 ) [0.0054701]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 247. Fare<=16    3/1 0 (0.667 0.333 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 275. Fare<=15.875    2/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 276. Fare>15.875    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 248. Fare>16    12/1 0 (0.917 0.083 ) [0.0138889]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 277. Fare<=18.39375    7/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 278. Fare>18.39375    5/1 0 (0.8 0.2 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 305. Fare<=19.3771    2/1 1 (0.5 0.5 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 306. Fare>19.3771    3/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |- 156. Fare>20.3875    2/1 1 (0.5 0.5 ) *\n",
      "|   |   |   |   |   |   |   |- 124. Fare>20.55    5/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |- 86. Fare>22.8875    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |- 50. Fare>23.35    25/1 0 (0.96 0.04 ) [0.0012444]\n",
      "|   |   |   |   |   |   |- 87. Fare<=25.9625    8/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |- 88. Fare>25.9625    17/1 0 (0.941 0.059 ) [0]\n",
      "|   |   |   |   |   |   |   |- 125. Fare<=26.125    15/1 0 (0.933 0.067 ) *\n",
      "|   |   |   |   |   |   |   |- 126. Fare>26.125    2/0 0 (1 0 ) *\n",
      "|   |   |   |   |- 26. Fare>26.26875    139/46 0 (0.669 0.331 ) [0.0197492]\n",
      "|   |   |   |   |   |- 51. Fare<=26.46875    4/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |- 52. Fare>26.46875    135/42 0 (0.689 0.311 ) [0.0098076]\n",
      "|   |   |   |   |   |   |- 89. Fare<=387.6646    133/40 0 (0.699 0.301 ) [0.0071421]\n",
      "|   |   |   |   |   |   |   |- 127. Fare<=134.64165    125/40 0 (0.68 0.32 ) [0.0151368]\n",
      "|   |   |   |   |   |   |   |   |- 157. Fare<=116.6375    122/37 0 (0.697 0.303 ) [0.0084306]\n",
      "|   |   |   |   |   |   |   |   |   |- 185. Fare<=52.2771    73/18 0 (0.753 0.247 ) [0.0234399]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 217. Fare<=36.8771    52/17 0 (0.673 0.327 ) [0.0185991]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 249. Fare<=35.0771    46/13 0 (0.717 0.283 ) [0.0252246]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 279. Fare<=31.1375    38/13 0 (0.658 0.342 ) [0.0290859]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 307. Fare<=29.85    26/6 0 (0.769 0.231 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 325. Fare<=26.775    13/6 0 (0.538 0.462 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 326. Fare>26.775    13/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 308. Fare>29.85    12/5 1 (0.417 0.583 ) [0.0277778]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 327. Fare<=30.5979    9/3 1 (0.333 0.667 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 341. Fare<=30.0354    4/1 1 (0.25 0.75 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 342. Fare>30.0354    5/2 1 (0.4 0.6 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 357. Fare<=30.2854    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 358. Fare>30.2854    4/1 1 (0.25 0.75 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 328. Fare>30.5979    3/1 0 (0.667 0.333 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 343. Fare<=30.8479    2/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 344. Fare>30.8479    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 280. Fare>31.1375    8/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 250. Fare>35.0771    6/2 1 (0.333 0.667 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 281. Fare<=36.125    4/1 1 (0.25 0.75 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 282. Fare>36.125    2/1 1 (0.5 0.5 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 218. Fare>36.8771    21/1 0 (0.952 0.048 ) [0.0145125]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 251. Fare<=51.93125    17/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 252. Fare>51.93125    4/1 0 (0.75 0.25 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 186. Fare>52.2771    49/19 0 (0.612 0.388 ) [0.0852978]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 219. Fare<=59.0875    15/4 1 (0.267 0.733 ) [0.0218803]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 253. Fare<=56.1979    7/3 1 (0.429 0.571 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 283. Fare<=52.8271    2/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 284. Fare>52.8271    5/2 0 (0.6 0.4 ) [0.0133333]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 309. Fare<=55.67085    4/2 1 (0.5 0.5 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 329. Fare<=54.27085    3/1 0 (0.667 0.333 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 330. Fare>54.27085    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 310. Fare>55.67085    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 254. Fare>56.1979    8/1 1 (0.125 0.875 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 285. Fare<=56.7125    6/1 1 (0.167 0.833 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 286. Fare>56.7125    2/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 220. Fare>59.0875    34/8 0 (0.765 0.235 ) [0.0111034]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 255. Fare<=75.1146    14/1 0 (0.929 0.071 ) [0.0027829]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 287. Fare<=64.97915    4/1 0 (0.75 0.25 ) [0.125]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 311. Fare<=62.66875    3/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 312. Fare>62.66875    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 288. Fare>64.97915    10/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 256. Fare>75.1146    20/7 0 (0.65 0.35 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 289. Fare<=77.00835    2/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 290. Fare>77.00835    18/5 0 (0.722 0.278 ) [0.0373457]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 313. Fare<=86.2896    9/1 0 (0.889 0.111 ) [0.0123457]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 331. Fare<=79.425    6/1 0 (0.833 0.167 ) [0.0277778]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 345. Fare<=79.025    3/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 346. Fare>79.025    3/1 0 (0.667 0.333 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 332. Fare>79.425    3/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 314. Fare>86.2896    9/4 0 (0.556 0.444 ) [0.0771605]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 333. Fare<=98.7521    4/1 1 (0.25 0.75 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 347. Fare<=89.5521    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 348. Fare>89.5521    3/1 1 (0.333 0.667 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 359. Fare<=90.5396    2/1 0 (0.5 0.5 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 360. Fare>90.5396    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 334. Fare>98.7521    5/1 0 (0.8 0.2 ) [0.02]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 349. Fare<=109.89165    2/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 350. Fare>109.89165    3/1 0 (0.667 0.333 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 361. Fare<=112.07915    2/1 1 (0.5 0.5 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 362. Fare>112.07915    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |- 158. Fare>116.6375    3/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |- 128. Fare>134.64165    8/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |- 90. Fare>387.6646    2/0 1 (0 1 ) *\n",
      "|   |   |- 6. Sex!='male'    248/53 1 (0.214 0.786 ) [0.0337842]\n",
      "|   |   |   |- 13. SibSp<=2.5    228/38 1 (0.167 0.833 ) [0.0223665]\n",
      "|   |   |   |   |- 27. Parch<=3.5    221/32 1 (0.145 0.855 ) [0.0186431]\n",
      "|   |   |   |   |   |- 53. Fare<=36.6875    131/30 1 (0.229 0.771 ) [0.0125136]\n",
      "|   |   |   |   |   |   |- 91. Fare<=33.6875    128/27 1 (0.211 0.789 ) [0.0106278]\n",
      "|   |   |   |   |   |   |   |- 129. Fare<=25.0396    93/24 1 (0.258 0.742 ) [0.0297149]\n",
      "|   |   |   |   |   |   |   |   |- 159. Fare<=14.12915    38/4 1 (0.105 0.895 ) [0.0002078]\n",
      "|   |   |   |   |   |   |   |   |   |- 187. Fare<=10.825    9/2 1 (0.222 0.778 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 221. Fare<=10.50835    8/1 1 (0.125 0.875 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 222. Fare>10.50835    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 188. Fare>10.825    29/2 1 (0.069 0.931 ) [0.0050066]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 223. Fare<=12.825    11/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 224. Fare>12.825    18/2 1 (0.111 0.889 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 257. Fare<=13.20835    14/2 1 (0.143 0.857 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 258. Fare>13.20835    4/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |- 160. Fare>14.12915    55/20 1 (0.364 0.636 ) [0.0890188]\n",
      "|   |   |   |   |   |   |   |   |   |- 189. Fare<=15.3729    13/3 0 (0.769 0.231 ) [0.0088757]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 225. Fare<=14.47915    8/1 0 (0.875 0.125 ) [0.0044643]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 259. Fare<=14.45625    6/1 0 (0.833 0.167 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 291. Fare<=14.4271    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 292. Fare>14.4271    5/1 0 (0.8 0.2 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 260. Fare>14.45625    2/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 226. Fare>14.47915    5/2 0 (0.6 0.4 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 261. Fare<=14.8729    2/1 1 (0.5 0.5 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 262. Fare>14.8729    3/1 0 (0.667 0.333 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 190. Fare>15.3729    42/10 1 (0.238 0.762 ) [0.0228647]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 227. Fare<=23.35    37/7 1 (0.189 0.811 ) [0.018717]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 263. Fare<=21.5125    29/7 1 (0.241 0.759 ) [0.0124508]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 293. Fare<=17.6    14/2 1 (0.143 0.857 ) [0.0091837]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 315. Fare<=16.4    11/2 1 (0.182 0.818 ) [0.0053129]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 335. Fare<=16.05    8/1 1 (0.125 0.875 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 351. Fare<=15.62085    4/1 1 (0.25 0.75 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 352. Fare>15.62085    4/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 336. Fare>16.05    3/1 1 (0.333 0.667 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 316. Fare>16.4    3/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 294. Fare>17.6    15/5 1 (0.333 0.667 ) [0.0634921]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 317. Fare<=18.375    3/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |- 318. Fare>18.375    12/2 1 (0.167 0.833 ) [0.0396825]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 337. Fare<=19.85625    7/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 338. Fare>19.85625    5/2 1 (0.4 0.6 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 353. Fare<=20.23125    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 354. Fare>20.23125    4/1 1 (0.25 0.75 ) [0.0416667]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 363. Fare<=20.7625    2/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |- 364. Fare>20.7625    2/1 1 (0.5 0.5 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 264. Fare>21.5125    8/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 228. Fare>23.35    5/2 0 (0.6 0.4 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 265. Fare<=23.725    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 266. Fare>23.725    4/2 1 (0.5 0.5 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 295. Fare<=24.075    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |   |- 296. Fare>24.075    3/1 0 (0.667 0.333 ) *\n",
      "|   |   |   |   |   |   |   |- 130. Fare>25.0396    35/3 1 (0.086 0.914 ) [0.0025994]\n",
      "|   |   |   |   |   |   |   |   |- 161. Fare<=28.85625    27/3 1 (0.111 0.889 ) [0.0123457]\n",
      "|   |   |   |   |   |   |   |   |   |- 191. Fare<=28.23125    26/2 1 (0.077 0.923 ) [0.0009862]\n",
      "|   |   |   |   |   |   |   |   |   |   |- 229. Fare<=26.125    16/2 1 (0.125 0.875 ) [0]\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 267. Fare<=25.9646    2/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |   |- 268. Fare>25.9646    14/2 1 (0.143 0.857 ) *\n",
      "|   |   |   |   |   |   |   |   |   |   |- 230. Fare>26.125    10/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |   |   |   |- 192. Fare>28.23125    1/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |   |   |   |- 162. Fare>28.85625    8/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |- 92. Fare>33.6875    3/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |- 54. Fare>36.6875    90/2 1 (0.022 0.978 ) [0.0036907]\n",
      "|   |   |   |   |   |   |- 93. Fare<=149.0354    73/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |- 94. Fare>149.0354    17/2 1 (0.118 0.882 ) [0]\n",
      "|   |   |   |   |   |   |   |- 131. Fare<=152.50625    3/1 0 (0.667 0.333 ) *\n",
      "|   |   |   |   |   |   |   |- 132. Fare>152.50625    14/0 1 (0 1 ) *\n",
      "|   |   |   |   |- 28. Parch>3.5    7/1 0 (0.857 0.143 ) [0.0306122]\n",
      "|   |   |   |   |   |- 55. Fare<=31.33125    4/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |- 56. Fare>31.33125    3/1 0 (0.667 0.333 ) [0]\n",
      "|   |   |   |   |   |   |- 95. Fare<=35.5375    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |- 96. Fare>35.5375    2/0 0 (1 0 ) *\n",
      "|   |   |   |- 14. SibSp>2.5    20/5 0 (0.75 0.25 ) [0.2179138]\n",
      "|   |   |   |   |- 29. Age<=19.5    14/1 0 (0.929 0.071 ) [0.0076531]\n",
      "|   |   |   |   |   |- 57. Fare<=31.33125    10/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |- 58. Fare>31.33125    4/1 0 (0.75 0.25 ) [0]\n",
      "|   |   |   |   |   |   |- 97. Fare<=39.14375    1/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |- 98. Fare>39.14375    3/0 0 (1 0 ) *\n",
      "|   |   |   |   |- 30. Age>19.5    6/2 1 (0.333 0.667 ) [0.1111111]\n",
      "|   |   |   |   |   |- 59. Fare<=166.275    4/2 0 (0.5 0.5 ) [0.1666667]\n",
      "|   |   |   |   |   |   |- 99. Fare<=23.23335    2/0 1 (0 1 ) *\n",
      "|   |   |   |   |   |   |- 100. Fare>23.23335    2/0 0 (1 0 ) *\n",
      "|   |   |   |   |   |- 60. Fare>166.275    2/0 1 (0 1 ) *\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree.fit(train.mapVars(\"Survived,Sex,Pclass,Embarked,Age,Fare,SibSp,Parch\"), \"Survived\");\n",
    "tree.printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Notice how large is the tree. Basically the tree was full grown and overfit the training data set too much. We can ask ourselves why that happens? Why it happens now, and did not happened when we had fewer inputs? The answer is that it happened also before. But it's consequences were not so drastic.\n",
    "\n",
    "The first tree used for training just $3$ input nominal features. Notice that all three features are nominal. The maximum number of groups which one can form is given by the product of the number of levels for each feature. This total maximal number is $2*3*3=18$. It practically exhausted the discrimination potential of those features. It did overfit in that reduced space of features. When we apply the model to the whole data set, the effect of exhaustion is not seen anymore.\n",
    "\n",
    "The second tree does the same thing, but this time in a richer space, with added input dimensions. Compared with the full feature space, we see the effect.\n",
    "\n",
    "There are two approaches to avoid overfit for a decision tree. The first approach is to stop learning up to the moment when we exhaust the data. The name for this approach is *early stop*. We can do that by specifying some parameters of the tree model:\n",
    "\n",
    "* Set a minimum number of instances for leaf node\n",
    "* Set a maximal depth for the tree\n",
    "* Not implemented yet, but easy to do: complexity threshold, maximal number of nodes in a tree\n",
    "\n",
    "The second approach is to prune the tree. Pruning procedure consists of growing the full tree and later on removing some nodes if they do not provide some type of gain. Currently we implemented only *reduced error pruning strategy*.\n",
    "\n",
    "We will test with 10-fold cross validation an early-stopping strategy to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\n",
      "CTree{maxDepth=8,minCount=4,purity=GiniGain,splitter=Random,varSelector=VarSelector[ALL]}\n",
      "CV score in training data\n",
      "=========================\n",
      "    dataset  metric    mean       std    \n",
      "[0]    test Accuracy 0.7867291 0.0434824 \n",
      "[1]   train Accuracy 0.8390089 0.0280662 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ClassifierEvaluation.cv(train.mapVars(\"Survived,Sex,Pclass,Embarked,Age,Fare,SibSp,Parch\"),\"Survived\",\n",
    "                        tree.maxDepth.set(8).minCount.set(4), 10, Accuracy.newMetric(true)).run().printContent();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I tried some values, just to show that we can do something about it, but the progress did not appear. We should try a different approach, and that is an ensemble. Next session contains directions on how to build such an ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CForest model\n",
    "\n",
    "Random forests are well-known to work well when the irreducible error from the training data is high. This is probably the case of this Titanic data set. We have reasons to believe that this is the situation since it was a tragedy. A lot of random or not-so-expected things happened. That happened despite of the bravery and the sacrifice of the crew and others.\n",
    "\n",
    "Random forests are the invention of [Leo Breiman](https://en.wikipedia.org/wiki/Leo_Breiman). The first design was a joint effort together with [Adele Cutler](http://www.math.usu.edu/adele/). The base of random forests is bagging (or **b**ootstrapp **ag**gregation). On top of that, selecting just a random limited number of variables at each node is the core of the algorithm.\n",
    "\n",
    "We will work with random forests for now. This ensemble is mode robust and is capable of obtaining much better results than a single tree. At the same time we will introduce 10-fold cross validation to check our progress and estimate the error produced.\n",
    "\n",
    "In the beginning we will use 10-fold cross validation for estimating the accuracy on public leader board.\n",
    "\n",
    "We will test first with 10 fold cv the tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\n",
      "CTree{purity=GiniGain,splitter=Random,varSelector=VarSelector[ALL]}\n",
      "CV score in training data\n",
      "=========================\n",
      "    dataset  metric    mean       std    \n",
      "[0]    test Accuracy 0.7856804 0.0277673 \n",
      "[1]   train Accuracy 0.7876299 0.0044525 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ClassifierEvaluation.cv(train.mapVars(\"Survived,Sex,Pclass,Embarked\"), \"Survived\", CTree.newCART(), 10, Accuracy.newMetric(true)).run().printContent();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Our first random forest\n",
    "\n",
    "The name of the random forest implementation is `CForest`. To build a new ensemble of trees, one have to instantiate it in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "CForest rf = CForest.newModel();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of things which can be customized for a random forest. Among them one can change:\n",
    "\n",
    "* Number of trees for classification\n",
    "* Which kind of weak classifierModel to use (you can customize this customized accordingly, like any other classifierModel)\n",
    "* Number of threads in pool (if you want to use parallelism)\n",
    "* What to do after each running step\n",
    "\n",
    "Let's build one and use ore new cross validation procedure to estimate it's error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\n",
      "CForest{rowSampler=Bootstrap(p=1),runs=100}\n",
      "CV score in training data\n",
      "=========================\n",
      "    dataset  metric    mean       std    \n",
      "[0]    test Accuracy 0.7835206 0.0656304 \n",
      "[1]   train Accuracy 0.7898753 0.0115865 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RandomSource.setSeed(123);\n",
    "Frame tr = train.mapVars(\"Survived,Sex,Pclass,Embarked\");\n",
    "CForest rf = CForest.newModel().runs.set(100);\n",
    "ClassifierEvaluation.cv(tr, \"Survived\", rf, 10, Accuracy.newMetric(true)).run().printContent();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, an identical output. This is due to the fact that our variables are already exhausted by the tree. It looks like an underfit. If one consider bias variance trade off, one can see this as high bias. We need to enrich our feature space to improve our performance.\n",
    "\n",
    "Let's be direct and test what would happen if we would use all our directly usable features? This time we will fit also the training data set, to see the distribution of the training error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\n",
      "CForest{rowSampler=Bootstrap(p=1),runs=100}\n",
      "CV score in training data\n",
      "=========================\n",
      "    dataset  metric    mean       std    \n",
      "[0]    test Accuracy 0.8115356 0.0490943 \n",
      "[1]   train Accuracy 0.9287949 0.00558   \n",
      "\n",
      "\n",
      "> Confusion matrix\n",
      " - Frequency table\n",
      "Ac\\Pr |    0    1 | total \n",
      "----- |    -    - | ----- \n",
      "    0 | >530   19 |   549 \n",
      "    1 |   45 >297 |   342 \n",
      "----- |    -    - | ----- \n",
      "total |  575  316 |   891 \n",
      " - Probability table\n",
      "Ac\\Pr |      0      1 | total \n",
      "----- |      -      - | ----- \n",
      "    0 | >0.595  0.021 | 0.616 \n",
      "    1 |  0.051 >0.333 | 0.384 \n",
      "----- |      -      - | ----- \n",
      "total |  0.645  0.355 | 1.000 \n",
      "\n",
      "\n",
      "Complete cases 891 from 891\n",
      "Acc: 0.9281706         (Accuracy )\n",
      "F1:  0.9430605         (F1 score / F-measure)\n",
      "MCC: 0.8475986         (Matthew correlation coefficient)\n",
      "Pre: 0.9217391         (Precision)\n",
      "Rec: 0.9653916         (Recall)\n",
      "G:   0.9433129         (G-measure)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RandomSource.setSeed(123);\n",
    "Frame tr = train.mapVars(\"Survived,Sex,Pclass,Embarked,Age,Fare,SibSp,Parch\");\n",
    "CForest rf = CForest.newModel().runs.set(100);\n",
    "ClassifierEvaluation.cv(tr, \"Survived\", rf, 10, Accuracy.newMetric(true)).run().printContent();\n",
    "\n",
    "rf.fit(tr, \"Survived\");\n",
    "ClassifierResult fit = rf.predict(test);\n",
    "Confusion.from(tr.rvar(\"Survived\"), rf.predict(tr).firstClasses()).printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we have a good example of overfit. Why is that? Look at the confusion matrix on the training set. We fit too well the training data. This data set is well known for its high irreducible error. And there is an explanation for that. During the tragic event a lot of exceptional things happened. For example I read somewhere that an old lady which had a dog was not allowed to embark with her pet due to regulations. As a consequence she decided to not leave it and she chose to die with him. It's close to impossible to learn those kind of things, even if the information would be available. \n",
    "\n",
    "We should reduce the error somehow. We can try to decrease the overfit by adding more learners. Let's see if that would be enough for our purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\n",
      "CForest{rowSampler=Bootstrap(p=1),runs=500}\n",
      "CV score in training data\n",
      "=========================\n",
      "    dataset  metric    mean       std    \n",
      "[0]    test Accuracy 0.8215855 0.0395714 \n",
      "[1]   train Accuracy 0.9311644 0.0036905 \n",
      "\n",
      "\n",
      "> Confusion matrix\n",
      " - Frequency table\n",
      "Ac\\Pr |    0    1 | total \n",
      "----- |    -    - | ----- \n",
      "    0 | >532   17 |   549 \n",
      "    1 |   46 >296 |   342 \n",
      "----- |    -    - | ----- \n",
      "total |  578  313 |   891 \n",
      " - Probability table\n",
      "Ac\\Pr |      0      1 | total \n",
      "----- |      -      - | ----- \n",
      "    0 | >0.597  0.019 | 0.616 \n",
      "    1 |  0.052 >0.332 | 0.384 \n",
      "----- |      -      - | ----- \n",
      "total |  0.649  0.351 | 1.000 \n",
      "\n",
      "\n",
      "Complete cases 891 from 891\n",
      "Acc: 0.9292929         (Accuracy )\n",
      "F1:  0.9440994         (F1 score / F-measure)\n",
      "MCC: 0.8501704         (Matthew correlation coefficient)\n",
      "Pre: 0.9204152         (Precision)\n",
      "Rec: 0.9690346         (Recall)\n",
      "G:   0.9444121         (G-measure)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RandomSource.setSeed(123);\n",
    "Frame tr = train.mapVars(\"Survived,Sex,Pclass,Embarked,Age,Fare,SibSp,Parch\");\n",
    "CForest rf = CForest.newModel().runs.set(500);\n",
    "ClassifierEvaluation.cv(tr, \"Survived\", rf, 10, Accuracy.newMetric(true)).run().printContent();\n",
    "\n",
    "rf.fit(tr, \"Survived\");\n",
    "ClassifierResult fit = rf.predict(test);\n",
    "Confusion.from(tr.rvar(\"Survived\"), rf.predict(tr).firstClasses()).printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is slightly better than before. But the difference does not look significantly better than previous. We will use a simple pre-pruning strategy is to limit the number instances in leaf nodes. We set the minimum count to $3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\n",
      "CForest{model=CTree,rowSampler=Bootstrap(p=1),runs=100}\n",
      "Raw scores:\n",
      "===========\n",
      "     dataset round fold Accuracy       dataset round fold Accuracy  \n",
      " [0]    test     0    0 0.7666667 [10]   train     0    0 0.906367  \n",
      " [1]    test     0    1 0.8202247 [11]   train     0    1 0.894015  \n",
      " [2]    test     0    2 0.8539326 [12]   train     0    2 0.9002494 \n",
      " [3]    test     0    3 0.8539326 [13]   train     0    3 0.8902743 \n",
      " [4]    test     0    4 0.8314607 [14]   train     0    4 0.8890274 \n",
      " [5]    test     0    5 0.8539326 [15]   train     0    5 0.8890274 \n",
      " [6]    test     0    6 0.8426966 [16]   train     0    6 0.8902743 \n",
      " [7]    test     0    7 0.7977528 [17]   train     0    7 0.8877805 \n",
      " [8]    test     0    8 0.8089888 [18]   train     0    8 0.8890274 \n",
      " [9]    test     0    9 0.741573  [19]   train     0    9 0.9014963 \n",
      "\n",
      "Round scores:\n",
      "=============\n",
      "    dataset round Accuracy_mean Accuracy_std \n",
      "[0]    test     0   0.8171161    0.0369175   \n",
      "[1]   train     0   0.8937539    0.0062306   \n",
      "\n",
      "CV score in training data\n",
      "=========================\n",
      "    dataset  metric    mean       std    \n",
      "[0]    test Accuracy 0.8171161 0.0389145 \n",
      "[1]   train Accuracy 0.8937539 0.0065677 \n",
      "\n",
      "\n",
      "> Confusion matrix\n",
      " - Frequency table\n",
      "Ac\\Pr |    0    1 | total \n",
      "----- |    -    - | ----- \n",
      "    0 | >515   34 |   549 \n",
      "    1 |   63 >279 |   342 \n",
      "----- |    -    - | ----- \n",
      "total |  578  313 |   891 \n",
      " - Probability table\n",
      "Ac\\Pr |      0      1 | total \n",
      "----- |      -      - | ----- \n",
      "    0 | >0.578  0.038 | 0.616 \n",
      "    1 |  0.071 >0.313 | 0.384 \n",
      "----- |      -      - | ----- \n",
      "total |  0.649  0.351 | 1.000 \n",
      "\n",
      "\n",
      "Complete cases 891 from 891\n",
      "Acc: 0.8911336         (Accuracy )\n",
      "F1:  0.9139308         (F1 score / F-measure)\n",
      "MCC: 0.7679856         (Matthew correlation coefficient)\n",
      "Pre: 0.8910035         (Precision)\n",
      "Rec: 0.9380692         (Recall)\n",
      "G:   0.9142335         (G-measure)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RandomSource.setSeed(123);\n",
    "Frame tr = train.mapVars(\"Survived,Sex,Pclass,Embarked,Age,Fare,SibSp,Parch\");\n",
    "CForest rf = CForest.newModel().model.set(CTree.newCART().minCount.set(3)).runs.set(100);\n",
    "ClassifierEvaluation.cv(tr, \"Survived\", rf, 10, Accuracy.newMetric(true)).run().printFullContent();\n",
    "\n",
    "rf.fit(tr, \"Survived\");\n",
    "ClassifierResult fit = rf.predict(test);\n",
    "Confusion.from(tr.rvar(\"Survived\"), rf.predict(tr).firstClasses()).printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we changed the classifierModel used by `CForest`. This is the same classifierModel used by default by random forest. We do this because we customized the classifierModel by changing the min count parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That had indeed some effect. However after submitting to competition we did not saw any improvement. We should look forward to engineer a little bit our features for further improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature engineering\n",
    "\n",
    "### 5.1 Title feature\n",
    "\n",
    "It is clear that we can't use directly the `\"Name\"` variable. This is due to the fact that names are almost unique, and that leads to a tiny generalization power. To understand that we should see that even if we learned that a passenger with a given name survived or not. We can't decide if another passenger survived, using only the name of the new passenger.\n",
    "\n",
    "Lets inspect some of the values from `\"Name\"` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VarNominal [name:\"Name\", rowCount:891]\n",
      " row                             value                            \n",
      "  [0]                  \"Braund, Mr. Owen Harris\"                  \n",
      "  [1]    \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\"    \n",
      "  [2]                  \"Heikkinen, Miss. Laina\"                   \n",
      "  [3]       \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\"        \n",
      "  [4]                 \"Allen, Mr. William Henry\"                  \n",
      "  [5]                     \"Moran, Mr. James\"                      \n",
      "  [6]                  \"McCarthy, Mr. Timothy J\"                  \n",
      "  [7]              \"Palsson, Master. Gosta Leonard\"               \n",
      "  [8]     \"Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\"     \n",
      "  [9]            \"Nasser, Mrs. Nicholas (Adele Achem)\"            \n",
      " [10]              \"Sandstrom, Miss. Marguerite Rut\"              \n",
      " [11]                 \"Bonnell, Miss. Elizabeth\"                  \n",
      " [12]              \"Saundercock, Mr. William Henry\"               \n",
      " [13]                \"Andersson, Mr. Anders Johan\"                \n",
      " [14]           \"Vestrom, Miss. Hulda Amanda Adolfina\"            \n",
      " [15]             \"Hewlett, Mrs. (Mary D Kingcome) \"              \n",
      " [16]                   \"Rice, Master. Eugene\"                    \n",
      " [17]               \"Williams, Mr. Charles Eugene\"                \n",
      " [18]  \"Vander Planke, Mrs. Julius (Emelia Maria Vandemoortele)\"  \n",
      " [19]                  \"Masselmani, Mrs. Fatima\"                  \n",
      " [20]                   \"Fynney, Mr. Joseph J\"                    \n",
      " [21]                   \"Beesley, Mr. Lawrence\"                   \n",
      " [22]               \"McGowan, Miss. Anna \"\"Annie\"\"\"               \n",
      " [23]               \"Sloper, Mr. William Thompson\"                \n",
      " [24]               \"Palsson, Miss. Torborg Danira\"               \n",
      " [25] \"Asplund, Mrs. Carl Oscar (Selma Augusta Emilia Johansson)\" \n",
      " [26]                  \"Emir, Mr. Farred Chehab\"                  \n",
      " [27]              \"Fortune, Mr. Charles Alexander\"               \n",
      " [28]              \"O'Dwyer, Miss. Ellen \"\"Nellie\"\"\"              \n",
      " [29]                    \"Todoroff, Mr. Lalio\"                    \n",
      " [30]                 \"Uruchurtu, Don. Manuel E\"                  \n",
      " [31]      \"Spencer, Mrs. William Augustus (Marie Eugenie)\"       \n",
      " [32]                 \"Glynn, Miss. Mary Agatha\"                  \n",
      " [33]                   \"Wheadon, Mr. Edward H\"                   \n",
      " [34]                  \"Meyer, Mr. Edgar Joseph\"                  \n",
      " [35]              \"Holverson, Mr. Alexander Oskar\"               \n",
      " [36]                     \"Mamee, Mr. Hanna\"                      \n",
      " [37]                 \"Cann, Mr. Ernest Charles\"                  \n",
      " [38]            \"Vander Planke, Miss. Augusta Maria\"             \n",
      " [39]                \"Nicola-Yarred, Miss. Jamila\"                \n",
      " [40]      \"Ahlin, Mrs. Johan (Johanna Persdotter Larsson)\"       \n",
      " [41] \"Turpin, Mrs. William John Robert (Dorothy Ann Wonnacott)\"  \n",
      " [42]                    \"Kraeff, Mr. Theodor\"                    \n",
      " [43]         \"Laroche, Miss. Simonne Marie Anne Andree\"          \n",
      " [44]               \"Devaney, Miss. Margaret Delia\"               \n",
      " [45]                 \"Rogers, Mr. William John\"                  \n",
      " [46]                     \"Lennon, Mr. Denis\"                     \n",
      " [47]                 \"O'Driscoll, Miss. Bridget\"                 \n",
      " [48]                    \"Samaan, Mr. Youssef\"                    \n",
      " [49]       \"Arnold-Franchi, Mrs. Josef (Josefine Franchi)\"       \n",
      " [50]                \"Panula, Master. Juha Niilo\"                 \n",
      " [51]               \"Nosworthy, Mr. Richard Cater\"                \n",
      " [52]         \"Harper, Mrs. Henry Sleeper (Myna Haxtun)\"          \n",
      " [53]    \"Faunthorpe, Mrs. Lizzie (Elizabeth Anne Wilkinson)\"     \n",
      " [54]              \"Ostby, Mr. Engelhart Cornelius\"               \n",
      " [55]                     \"Woolner, Mr. Hugh\"                     \n",
      " [56]                     \"Rugg, Miss. Emily\"                     \n",
      " [57]                    \"Novel, Mr. Mansouer\"                    \n",
      " [58]               \"West, Miss. Constance Mirium\"                \n",
      " [59]            \"Goodwin, Master. William Frederick\"             \n",
      " [60]                   \"Sirayanian, Mr. Orsen\"                   \n",
      " [61]                    \"Icard, Miss. Amelie\"                    \n",
      " [62]                \"Harris, Mr. Henry Birkhardt\"                \n",
      " [63]                   \"Skoog, Master. Harald\"                   \n",
      " [64]                   \"Stewart, Mr. Albert A\"                   \n",
      " [65]                 \"Moubarek, Master. Gerios\"                  \n",
      " [66]               \"Nye, Mrs. (Elizabeth Ramell)\"                \n",
      " [67]                 \"Crease, Mr. Ernest James\"                  \n",
      " [68]              \"Andersson, Miss. Erna Alexandra\"              \n",
      " [69]                     \"Kink, Mr. Vincenz\"                     \n",
      " [70]                \"Jenkin, Mr. Stephen Curnow\"                 \n",
      " [71]                \"Goodwin, Miss. Lillian Amy\"                 \n",
      " [72]                   \"Hood, Mr. Ambrose Jr\"                    \n",
      " [73]                \"Chronopoulos, Mr. Apostolos\"                \n",
      " [74]                       \"Bing, Mr. Lee\"                       \n",
      " [75]                  \"Moen, Mr. Sigurd Hansen\"                  \n",
      " [76]                     \"Staneff, Mr. Ivan\"                     \n",
      " [77]                 \"Moutal, Mr. Rahamin Haim\"                  \n",
      " [78]               \"Caldwell, Master. Alden Gates\"               \n",
      " ...                              ...                             \n",
      "[871]     \"Beckwith, Mrs. Richard Leonard (Sallie Monypeny)\"      \n",
      "[872]                 \"Carlsson, Mr. Frans Olof\"                  \n",
      "[873]                \"Vander Cruyssen, Mr. Victor\"                \n",
      "[874]           \"Abelson, Mrs. Samuel (Hannah Wizosky)\"           \n",
      "[875]            \"Najib, Miss. Adele Kiamie \"\"Jane\"\"\"             \n",
      "[876]               \"Gustafsson, Mr. Alfred Ossian\"               \n",
      "[877]                   \"Petroff, Mr. Nedelio\"                    \n",
      "[878]                    \"Laleff, Mr. Kristo\"                     \n",
      "[879]       \"Potter, Mrs. Thomas Jr (Lily Alexenia Wilson)\"       \n",
      "[880]       \"Shelley, Mrs. William (Imanita Parrish Hall)\"        \n",
      "[881]                    \"Markun, Mr. Johann\"                     \n",
      "[882]               \"Dahlberg, Miss. Gerda Ulrika\"                \n",
      "[883]               \"Banfield, Mr. Frederick James\"               \n",
      "[884]                  \"Sutehall, Mr. Henry Jr\"                   \n",
      "[885]           \"Rice, Mrs. William (Margaret Norton)\"            \n",
      "[886]                   \"Montvila, Rev. Juozas\"                   \n",
      "[887]               \"Graham, Miss. Margaret Edith\"                \n",
      "[888]        \"Johnston, Miss. Catherine Helen \"\"Carrie\"\"\"         \n",
      "[889]                   \"Behr, Mr. Karl Howell\"                   \n",
      "[890]                    \"Dooley, Mr. Patrick\"                    \n",
      "                                                                  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.rvar(\"Name\").printContent();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the names contains title information of the individual. This is valuable, but how can we benefit from that? First of all see that the format of that string is clear: space + title + dot + space. We can try to model a regular expression or we can take a simpler, but manual path. Intuition tells us that there should not be too many keys.\n",
    "\n",
    "We build a set with known keys. After that we filter out names with known titles, and print first twenty of them. We see that we have already `\"Mrs\"` and `\"Mr\"`. Let's find others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Heikkinen, Miss. Laina\"\n",
      "\"Palsson, Master. Gosta Leonard\"\n",
      "\"Sandstrom, Miss. Marguerite Rut\"\n",
      "\"Bonnell, Miss. Elizabeth\"\n",
      "\"Vestrom, Miss. Hulda Amanda Adolfina\"\n",
      "\"Rice, Master. Eugene\"\n",
      "\"McGowan, Miss. Anna \"\"Annie\"\"\"\n",
      "\"Palsson, Miss. Torborg Danira\"\n",
      "\"O'Dwyer, Miss. Ellen \"\"Nellie\"\"\"\n",
      "\"Uruchurtu, Don. Manuel E\"\n",
      "\"Glynn, Miss. Mary Agatha\"\n",
      "\"Vander Planke, Miss. Augusta Maria\"\n",
      "\"Nicola-Yarred, Miss. Jamila\"\n",
      "\"Laroche, Miss. Simonne Marie Anne Andree\"\n",
      "\"Devaney, Miss. Margaret Delia\"\n",
      "\"O'Driscoll, Miss. Bridget\"\n",
      "\"Panula, Master. Juha Niilo\"\n",
      "\"Rugg, Miss. Emily\"\n",
      "\"West, Miss. Constance Mirium\"\n",
      "\"Goodwin, Master. William Frederick\"\n"
     ]
    }
   ],
   "source": [
    "// build incrementally a set with known keys\n",
    "HashSet<String> keys = new HashSet<>();\n",
    "keys.add(\"Mrs\");\n",
    "keys.add(\"Mr\");\n",
    "\n",
    "// filter out names with known keys\n",
    "// print first twenty to inspect and see other keys\n",
    "train.rvar(\"Name\").stream().mapToString().filter(txt -> {\n",
    "    for(String key : keys)\n",
    "        if(txt.contains(\" \" + key + \". \"))\n",
    "            return false;\n",
    "    return true;\n",
    "}).limit(20).forEach(WS::println);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reduced our search and found other titles like `\"Miss\"`, `\"Master\"`. We arrive at the following set of keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?  Mr Mrs Miss Master Don Rev Dr Mme Ms Major Lady Sir Mlle Col Capt Countess Jonkheer \n",
      "-  -- --- ---- ------ --- --- -- --- -- ----- ---- --- ---- --- ---- -------- -------- \n",
      "0 517 125 182    40    1   6  7   1  1    2    1    1   2    2   1      1        1     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "HashSet<String> keys = new HashSet<>();\n",
    "keys.addAll(Arrays.asList(\"Mrs\", \"Mme\", \"Lady\", \"Countess\", \"Mr\", \"Sir\",\"Don\", \"Ms\", \"Miss\", \n",
    "\"Mlle\", \"Master\", \"Dr\",\"Col\", \"Major\", \"Jonkheer\", \"Capt\", \"Rev\"));\n",
    "\n",
    "VarNominal title = train.rvar(\"Name\").stream().mapToString().map(txt -> {\n",
    "    for(String key : keys)\n",
    "        if(txt.contains(\" \" + key + \". \"))\n",
    "            return key;\n",
    "    return \"?\";\n",
    "}).collect(VarNominal.collector());\n",
    "DensityVector.fromLevelCounts(true, title).printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that we exhausted training data. This is enough. It is possible that in test data to appear new titles. We will consider them missing values. That is why we return `\"?\"` when no matching is found. Another thing to notice is that some of the labels have few number of appearances. We will merge them in a greater category.\n",
    "\n",
    "Another useful feature built in `rapaio` is filters. There are two types of filters: variable filters and frame filters. The nice part of frame filters is that learning algorithms are able to use frame filters naturally, in order to make feature transformations on data. This kind of filters are called *input filters* from the learning algorithm perspective. It is important that you know that input filters transforms features before train phase and also on fit phase.\n",
    "\n",
    "We will build a learning filter to create a new feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitleFilter implements FFilter {\n",
    "    private HashMap<String, String[]> replaceMap = new HashMap<>();\n",
    "    private Function<String, String> titleFun = txt -> {\n",
    "        for (Map.Entry<String, String[]> e : replaceMap.entrySet()) {\n",
    "            for (int i = 0; i < e.getValue().length; i++) {\n",
    "                if (txt.contains(\" \" + e.getValue()[i] + \". \"))\n",
    "                    return e.getKey();\n",
    "                }\n",
    "            }\n",
    "        return \"?\";\n",
    "    };\n",
    "\n",
    "    @Override\n",
    "    public void fit(Frame df) {\n",
    "        replaceMap.put(\"Mrs\", new String[]{\"Mrs\", \"Mme\", \"Lady\", \"Countess\"});\n",
    "        replaceMap.put(\"Mr\", new String[]{\"Mr\", \"Sir\", \"Don\", \"Ms\"});\n",
    "        replaceMap.put(\"Miss\", new String[]{\"Miss\", \"Mlle\"});\n",
    "        replaceMap.put(\"Master\", new String[]{\"Master\"});\n",
    "        replaceMap.put(\"Dr\", new String[]{\"Dr\"});\n",
    "        replaceMap.put(\"Military\", new String[]{\"Col\", \"Major\", \"Jonkheer\", \"Capt\"});\n",
    "        replaceMap.put(\"Rev\", new String[]{\"Rev\"});\n",
    "    }\n",
    "\n",
    "    @Override\n",
    "    public Frame apply(Frame df) {\n",
    "        VarNominal title = VarNominal.empty(0, new ArrayList<>(replaceMap.keySet())).name(\"Title\");\n",
    "        df.rvar(\"Name\").stream().mapToString().forEach(name -> title.addLabel(titleFun.apply(name)));\n",
    "        return df.bindVars(title);\n",
    "    }\n",
    "    \n",
    "    @Override\n",
    "    public TitleFilter newInstance() {\n",
    "        return new TitleFilter();\n",
    "    }\n",
    "    \n",
    "    public String[] varNames() {\n",
    "        return new String[0];\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try a new random forest on the reduced data set and also on title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\n",
      "CForest{model=CTree,rowSampler=Bootstrap(p=1),runs=100}\n",
      "CV score in training data\n",
      "=========================\n",
      "    dataset  metric    mean       std    \n",
      "[0]    test Accuracy 0.8125218 0.0384124 \n",
      "[1]   train Accuracy 0.892505  0.0043195 \n",
      "\n",
      "\n",
      "CForest\n",
      "=======\n",
      "\n",
      "Description:\n",
      "CForest{model=CTree,rowSampler=Bootstrap(p=1),runs=100}\n",
      "\n",
      "Capabilities:\n",
      "types inputs/targets: NOMINAL,INT,DOUBLE,BINARY/NOMINAL\n",
      "counts inputs/targets: [1,1000000] / [1,1]\n",
      "missing inputs/targets: true/false\n",
      "\n",
      "Model fitted: true.\n",
      "Learned model:\n",
      "input vars: \n",
      "0.      Sex : NOMINAL  | \n",
      "1.   Pclass : NOMINAL  | \n",
      "2. Embarked : NOMINAL  | \n",
      "3.    Title : NOMINAL  | \n",
      "\n",
      "target vars:\n",
      "> Survived : NOMINAL [?,0,1]\n",
      "\n",
      "\n",
      "Fitted trees:100\n",
      "oob enabled:false\n",
      "\n",
      "> Confusion matrix\n",
      " - Frequency table\n",
      "Ac\\Pr |    0    1 | total \n",
      "----- |    -    - | ----- \n",
      "    0 | >467   82 |   549 \n",
      "    1 |  102 >240 |   342 \n",
      "----- |    -    - | ----- \n",
      "total |  569  322 |   891 \n",
      " - Probability table\n",
      "Ac\\Pr |      0      1 | total \n",
      "----- |      -      - | ----- \n",
      "    0 | >0.524  0.092 | 0.616 \n",
      "    1 |  0.114 >0.269 | 0.384 \n",
      "----- |      -      - | ----- \n",
      "total |  0.639  0.361 | 1.000 \n",
      "\n",
      "\n",
      "Complete cases 891 from 891\n",
      "Acc: 0.7934905         (Accuracy )\n",
      "F1:  0.8354204         (F1 score / F-measure)\n",
      "MCC: 0.559194         (Matthew correlation coefficient)\n",
      "Pre: 0.8207381         (Precision)\n",
      "Rec: 0.8506375         (Recall)\n",
      "G:   0.8355541         (G-measure)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RandomSource.setSeed(124);\n",
    "\n",
    "Frame df_train = train.fapply(new TitleFilter());\n",
    "Frame df_test = test.fapply(new TitleFilter());\n",
    "\n",
    "CForest rf = CForest.newModel().model.set(CTree.newCART().minCount.set(3)).runs.set(100);\n",
    "\n",
    "ClassifierEvaluation.cv(df_train.mapVars(\"Survived,Sex,Pclass,Embarked,Age,Fare,SibSp,Parch\"), \"Survived\", rf, 10, Accuracy.newMetric(true)).run().printContent();\n",
    "\n",
    "rf.fit(df_train.mapVars(\"Survived,Sex,Pclass,Embarked,Title\"), \"Survived\");\n",
    "rf.printSummary();\n",
    "\n",
    "ClassifierResult fit = rf.predict(df_test);\n",
    "Confusion.from(train.rvar(\"Survived\"), rf.predict(df_train).firstClasses()).printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that looks slightly better than our best classifierModel. We submit that to kaggle to see if there is any improvement.\n",
    "\n",
    "![Progress after incorporating title name into input features](images/titanic-rf1-model.png)\n",
    "\n",
    "### 5.2 Other features\n",
    "\n",
    "There are various authors which published their work on solving this kaggle competition. Most interesting part of their work is the feature engineering section. I developed here some ideas in order to show how one can do this with the library.\n",
    "\n",
    "#### 5.2.1 Family size\n",
    "\n",
    "Using directly `\"SibSp\"` and `\"Parch\"` fields yields no value for a random forest classifierModel. Studying this two features it looks like those values can be both combined into a single one by summation. This would give us a family size estimator.\n",
    "\n",
    "In order to have an idea of the performance of this new estimator I used a ChiSquare independence test. The idea is to study if those features taken separately worth less than combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> ChiSqIndependence\n",
      "\n",
      "Pearson's Chi-squared test with Yates' continuity correction\n",
      "\n",
      "X-squared = 31.9279202, df = 6, p-value = 0.0000168\n",
      "\n",
      "Observed data:\n",
      "      1.0 0.0 3.0 4.0 2.0 5.0 8.0 total \n",
      "    0  97 398 12  15  15   5   7   549  \n",
      "    1 112 210  4   3  13   0   0   342  \n",
      "total 209 608 16  18  28   5   7   891  \n",
      "\n",
      "Expected data:\n",
      "              1.0         0.0        3.0        4.0        2.0       5.0       8.0 total \n",
      "    0 128.7777778 374.6262626  9.8585859 11.0909091 17.2525253 3.0808081 4.3131313  549  \n",
      "    1  80.2222222 233.3737374  6.1414141  6.9090909 10.7474747 1.9191919 2.6868687  342  \n",
      "total 209         608         16         18         28         5         7          891  \n",
      "\n",
      "\n",
      "> ChiSqIndependence\n",
      "\n",
      "Pearson's Chi-squared test with Yates' continuity correction\n",
      "\n",
      "X-squared = 23.3892671, df = 6, p-value = 0.0006761\n",
      "\n",
      "Observed data:\n",
      "      0.0 1.0 2.0 5.0 3.0 4.0 6.0 total \n",
      "    0 445  53 40   4   2   4   1   549  \n",
      "    1 233  65 40   1   3   0   0   342  \n",
      "total 678 118 80   5   5   4   1   891  \n",
      "\n",
      "Expected data:\n",
      "              0.0         1.0        2.0       5.0       3.0       4.0       6.0 total \n",
      "    0 417.7575758  72.7070707 49.2929293 3.0808081 3.0808081 2.4646465 0.6161616  549  \n",
      "    1 260.2424242  45.2929293 30.7070707 1.9191919 1.9191919 1.5353535 0.3838384  342  \n",
      "total 678         118         80         5         5         4         1          891  \n",
      "\n",
      "\n",
      "> ChiSqIndependence\n",
      "\n",
      "Pearson's Chi-squared test with Yates' continuity correction\n",
      "\n",
      "X-squared = 72.6627665, df = 8, p-value =   1.45e-12\n",
      "\n",
      "Observed data:\n",
      "        2   1  5   3  7  6  4 8 11 total \n",
      "    0  72 374 12  43  8 19  8 6 7   549  \n",
      "    1  89 163  3  59  4  3 21 0 0   342  \n",
      "total 161 537 15 102 12 22 29 6 7   891  \n",
      "\n",
      "Expected data:\n",
      "                2           1          5           3          7          6          4 \n",
      "    0  99.2020202 330.8787879  9.2424242  62.8484848  7.3939394 13.5555556 17.8686869 \n",
      "    1  61.7979798 206.1212121  5.7575758  39.1515152  4.6060606  8.4444444 11.1313131 \n",
      "total 161         537         15         102         12         22         29         \n",
      "\n",
      "        8        11 total \n",
      "3.6969697 4.3131313  549  \n",
      "2.3030303 2.6868687  342  \n",
      "6         7          891  \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// convert sibsp and parch to nominal types to be able to use a chi-square test\n",
    "Var sibsp = VarNominal.from(train.rowCount(), row -> train.getLabel(row, \"SibSp\"));\n",
    "Var parch = VarNominal.from(train.rowCount(), row -> train.getLabel(row, \"Parch\"));\n",
    "\n",
    "// test individually each feature\n",
    "ChiSqIndependence.from(train.rvar(\"Survived\"), sibsp, true).printSummary();\n",
    "ChiSqIndependence.from(train.rvar(\"Survived\"), parch, true).printSummary();\n",
    "\n",
    "// build a combined feature by summation, as nominal\n",
    "VarNominal familySize = VarNominal.from(train.rowCount(),\n",
    "row -> \"\" + (1 + train.getInt(row, \"SibSp\") + train.getInt(row, \"Parch\")));\n",
    "\n",
    "// run the chi-square test on sumation\n",
    "ChiSqIndependence.from(train.rvar(\"Survived\"), familySize, true).printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How we can interpret the result? The test says that each feature brings value separately. The *p-value* associated with both test could be considered significant. That means there are string evidence that those features are not independent of target class. As a conclusion, those features are useful. The last test is made for their summation. It looks like he test is more significant than the previous two. As a consequence we can use the summation instead of those two values taken independently.\n",
    "\n",
    "### 5.3 Cabin and Ticket\n",
    "\n",
    "It seems that cabin and ticket denominations are not useful as they are. There are some various reasons why not to do so. First of all they have many missing values. But a stringer reason is that both have too many levels to contain solid generalization base for learning.\n",
    "\n",
    "If we take only the first letter from each of those two fields, more generalization can happen. This is probably due to the fact that perhaps there is some localization information encoded in those. Perhaps information about the deck, the comfort level, auxiliary functions is encoded in there. As a conclusion it worth a try so we proceed with this thing.\n",
    "\n",
    "To combine all those things we can do it in a single filter or into many filters applied on data. I chose to do a single custom filter to solve all those problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFilter implements FFilter {\n",
    "\n",
    "    private HashMap<String, String[]> replaceMap = new HashMap<>();\n",
    "    private Function<String, String> titleFun = txt -> {\n",
    "        for (Map.Entry<String, String[]> e : replaceMap.entrySet()) {\n",
    "            for (int i = 0; i < e.getValue().length; i++) {\n",
    "                if (txt.contains(\" \" + e.getValue()[i] + \". \"))\n",
    "                    return e.getKey();\n",
    "            }\n",
    "        }\n",
    "        return \"?\";\n",
    "    };\n",
    "\n",
    "    public void fit(Frame df) {\n",
    "        replaceMap.put(\"Mrs\", new String[]{\"Mrs\", \"Mme\", \"Lady\", \"Countess\"});\n",
    "        replaceMap.put(\"Mr\", new String[]{\"Mr\", \"Sir\", \"Don\", \"Ms\"});\n",
    "        replaceMap.put(\"Miss\", new String[]{\"Miss\", \"Mlle\"});\n",
    "        replaceMap.put(\"Master\", new String[]{\"Master\"});\n",
    "        replaceMap.put(\"Dr\", new String[]{\"Dr\"});\n",
    "        replaceMap.put(\"Military\", new String[]{\"Col\", \"Major\", \"Jonkheer\", \"Capt\"});\n",
    "        replaceMap.put(\"Rev\", new String[]{\"Rev\"});\n",
    "    }\n",
    "\n",
    "    public Frame apply(Frame df) {\n",
    "        VarNominal title = VarNominal.empty(0, new ArrayList<>(replaceMap.keySet())).name(\"Title\");\n",
    "        df.rvar(\"Name\").stream().mapToString().forEach(name -> title.addLabel(titleFun.apply(name)));\n",
    "\n",
    "        Var famSize = VarDouble.from(df.rowCount(), row ->\n",
    "        1.0 + df.getInt(row, \"SibSp\") + df.getInt(row, \"Parch\")).name(\"FamilySize\");\n",
    "\n",
    "        Var ticket = VarNominal.from(df.rowCount(), row ->\n",
    "        df.isMissing(row, \"Ticket\") ? \"?\" : df.getLabel(row, \"Ticket\")\n",
    "        .substring(0, 1).toUpperCase()\n",
    "        ).name(\"Ticket\");\n",
    "\n",
    "        Var cabin = VarNominal.from(df.rowCount(), row ->\n",
    "        df.isMissing(row, \"Cabin\") ? \"?\" : (df.getLabel(row, \"Cabin\")\n",
    "        .substring(0, 1).toUpperCase())\n",
    "        ).name(\"Cabin\");\n",
    "\n",
    "        return df.removeVars(\"Ticket,Cabin\").bindVars(famSize, ticket, cabin, title).copy();\n",
    "    }\n",
    "\n",
    "    public CustomFilter newInstance() { return new CustomFilter(); }\n",
    "    \n",
    "    public String[] varNames() { return new String[0]; }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Another try with random forests\n",
    "\n",
    "So we have some new features and we look to learn from them. We can use a previous classifierModel like random forests to test it before submit.\n",
    "\n",
    "But we know that we are in danger to overfit is rf. One idea is to transform numeric features into nominal ones by a process named discretization. For this purpose we use a filter from the library called `FQuantileDiscrete`. This filter computes a given number of quantile intervals and put labels according with those intervals on numerical values. Let's see how we proceed and how the data looks like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Summary\n",
      "=============\n",
      "* rowCount: 891\n",
      "* complete: 183/891\n",
      "* varCount: 12\n",
      "* varNames: \n",
      "\n",
      " 0.   Survived : nom |  4.      SibSp : nom |  8. FamilySize : nom | \n",
      " 1.     Pclass : nom |  5.      Parch : nom |  9.     Ticket : nom | \n",
      " 2.        Sex : nom |  6.       Fare : nom | 10.      Cabin : nom | \n",
      " 3.        Age : nom |  7.   Embarked : nom | 11.      Title : nom | \n",
      "\n",
      "* summary: \n",
      " Survived [nom]  Pclass [nom]      Sex [nom]       Age [nom]    SibSp [nom]    Parch [nom] \n",
      "      0 :   549     3 :   491   male :   577 31.8~36 :    91 -Inf~0 :   608 -Inf~0 :   678 \n",
      "      1 :   342     1 :   216 female :   314   14~19 :    87  0~Inf :   283  0~Inf :   213 \n",
      "                    2 :   184                  41~50 :    78                               \n",
      "                                             -Inf~14 :    77                               \n",
      "                                             (Other) :   381                               \n",
      "                                                 NAs :   177                               \n",
      "                                                                                           \n",
      "\n",
      "           Fare [nom]  Embarked [nom]  FamilySize [nom]    Ticket [nom]     Cabin [nom] \n",
      "   7.854~8.05 :   106       S :   644    -Inf~1 :   537       3 :   301       C :    59 \n",
      "    -Inf~7.55 :    92       C :   168       1~2 :   161       2 :   183       B :    47 \n",
      "    27~39.688 :    91       Q :    77       2~3 :   102       1 :   146       D :    33 \n",
      "39.688~77.958 :    89                     3~Inf :    91       P :    65       E :    32 \n",
      "    21.679~27 :    89                                         S :    65 (Other) :    33 \n",
      "      (Other) :   424     NAs :     2                   (Other) :   131     NAs :   687 \n",
      "                                                                                        \n",
      "\n",
      "    Title [nom] \n",
      "     Mr :   520 \n",
      "   Miss :   184 \n",
      "    Mrs :   128 \n",
      " Master :    40 \n",
      "     Dr :     7 \n",
      "(Other) :    12 \n",
      "                \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FFilter[] inputFilters = new FFilter[]{\n",
    "new CustomFilter(),\n",
    "FQuantileDiscrete.split(VarRange.of(\"Age\"), 10),\n",
    "FQuantileDiscrete.split(VarRange.of(\"Fare\"), 10),\n",
    "FQuantileDiscrete.split(VarRange.of(\"SibSp\"), 3),\n",
    "FQuantileDiscrete.split(VarRange.of(\"Parch\"), 3),\n",
    "FQuantileDiscrete.split(VarRange.of(\"FamilySize\"), 8),\n",
    "FRemoveVars.remove(VarRange.of(\"PassengerId,Name\"))\n",
    "};\n",
    "\n",
    "// print a summary of the transformed data\n",
    "train.fapply(inputFilters).printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `\"Age\"` values are now intervals and still $177$ missing values.\n",
    "\n",
    "The values chosen for quantile numbers is more or less arbitrary. There is no *good* numbers in general, only for some specific purposes.\n",
    "\n",
    "As promised, we will give a try to another random forest to see if it can better generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "EvalException",
     "evalue": "Cannot invoke \"rapaio.data.Var.stream()\" because the return value of \"rapaio.data.Frame.rvar(String)\" is null",
     "output_type": "error",
     "traceback": [
      "\u001b[1m\u001b[31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1m\u001b[31mjava.lang.NullPointerException: Cannot invoke \"rapaio.data.Var.stream()\" because the return value of \"rapaio.data.Frame.rvar(String)\" is null\u001b[0m",
      "\u001b[1m\u001b[31m\tat CustomFilter.apply(#159:26)\u001b[0m",
      "\u001b[1m\u001b[31m\tat rapaio.data.filter.FFilter.fapply(FFilter.java:70)\u001b[0m",
      "\u001b[1m\u001b[31m\tat rapaio.data.Frame.fapply(Frame.java:512)\u001b[0m",
      "\u001b[1m\u001b[31m\tat .(#168:1)\u001b[0m"
     ]
    }
   ],
   "source": [
    "RandomSource.setSeed(123);\n",
    "CForest rf3 = CForest.newModel().runs.set(200);\n",
    "\n",
    "train = train.fapply(inputFilters);\n",
    "\n",
    "rf3.fit(train, \"Survived\");\n",
    "ClassifierEvaluation.cv(train, \"Survived\", rf3, 10).run().printContent();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried some ideas to make the forest to generalize better.\n",
    "\n",
    "* Smaller bootstrap percentage - this could lead to increased independence of trees\n",
    "* Use `GainRatio` as purity function because sometimes is more conservative\n",
    "* Use `MinGain` to avoid growing trees to have many leaves with a single instance\n",
    "* Use `mCols=4`, number of variables used for testing - more than default value, to improve the quality of each tree\n",
    "\n",
    "These are the results. At a first look might seem like an astonishing result. But we know that the irreducible error for this data set is high and is close to $$0.2$$. It seems obvious that we failed to reduce the variance and we still overfit a lot using this construct. Since this is a tutorial I will not insist on improving this model, but I think that even if it would be improved, the gain would be very small. Perhaps another approach would be better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. SVM model\n",
    "\n",
    "SVM (Support Vector Machines) is a nice framework to test new ideas for various types of problems. The power of SMVs comes from their kernels. A kernel is basically a transformation of the original space generated by the input features into another space, often with more dimensions. It's like a feature engineering in a singe function.\n",
    "\n",
    "But SVMs have a practical problem. The features needs to be numeric and does not allows missing values. This is not a constrain on the algorithm itself. At any moment one can build a kernel for nominal features. But the implemented ones allows only numeric non missing values and is much simpler to shape our data into this format.\n",
    "\n",
    "How can we do that?\n",
    "\n",
    "### 6.1 Data preparation\n",
    "\n",
    "We can use a filter to impute data for missing values. The filter we use is an imputation with a classifierModel of imputation with a regression. The logic is the following: train a classifierModel from a specified set of input features to predict the field with missing values. The data set inside the filter is filtered to contain only instances with non-missing target values.\n",
    "\n",
    "After we impute the missing values we encode nominal features into numeric features. We can accomplish this task using, again, another filter for this purpose. The name of this filter is `FOneHotEncoding`. What it does is to create a number of numeric feature for level of the nominal variable. Than the values on these numeric variables receives the value of the indicator function. We have $1$ if the level equals the numeric variable's name, $0$ otherwise.\n",
    "\n",
    "After we have numerical variables, is better to make all the variables to be in the same range. This is not a requirement for SMVs in general. The meaning is to give same weight to all the involved variables. As a side effect it makes the algorithm to run faster. This is due to the fact that the convex optimization problem has smaller chances to have a close-to-flat big surfaces.\n",
    "\n",
    "Finally, we will remove the not used variables from the frame in order to be prepared for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "EvalException",
     "evalue": "null",
     "output_type": "error",
     "traceback": [
      "\u001b[1m\u001b[31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1m\u001b[31mjava.lang.NullPointerException: null\u001b[0m",
      "\u001b[1m\u001b[31m\tat CustomFilter.apply(#129:26)\u001b[0m",
      "\u001b[1m\u001b[31m\tat rapaio.data.filter.FFilter.fapply(FFilter.java:76)\u001b[0m",
      "\u001b[1m\u001b[31m\tat rapaio.data.Frame.fapply(Frame.java:518)\u001b[0m",
      "\u001b[1m\u001b[31m\tat .(#146:1)\u001b[0m"
     ]
    }
   ],
   "source": [
    "FFilter[] inputFilters = new FFilter[]{\n",
    "new CustomFilter(),\n",
    "FImputeRegression.of(RForest.newRF().runs.set(100),VRange.of(\"Age,Pclass,Embarked,Sex,Fare,Title\"), \"Age\"),\n",
    "FImputeClassifier.of(CForest.newModel().runs.set(10),VRange.of(\"Embarked,Age,Pclass,Sex,Title\"), \"Embarked\"),\n",
    "FImputeClassifier.of(CForest.newModel().runs.set(100),VRange.of(\"Age,Pclass,Embarked,Sex,Fare,Ticket\"), \"Ticket\"),\n",
    "FImputeClassifier.of(CForest.newModel().runs.set(100),VRange.of(\"Age,Pclass,Embarked,Sex,Fare,Cabin\"), \"Cabin\"),\n",
    "FOneHotEncoding.on(\"Sex,Embarked,Title,Cabin\"),\n",
    "FOneHotEncoding.on(\"Ticket\"),\n",
    "FStandardize.on(VRange.all()),\n",
    "FRemoveVars.remove(VRange.of(\"PassengerId,Name,SibSp,Parch\"))\n",
    "};\n",
    "train.fapply(inputFilters).printSummary();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```java\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "Frame Summary\n",
    "=============\n",
    "* rowCount: 891\n",
    "* complete: 891/891\n",
    "* varCount: 41\n",
    "* varNames:\n",
    "\n",
    "0. Survived : nom | 14. Ticket.3 : num | 28. Cabin.G : num |\n",
    "1. Pclass : num | 15. Ticket.2 : num | 29. Cabin.D : num |\n",
    "2. Sex.male : num | 16. Ticket.C : num | 30. Cabin.A : num |\n",
    "3. Sex.female : num | 17. Ticket.7 : num | 31. Cabin.B : num |\n",
    "4. Age : num | 18. Ticket.W : num | 32. Cabin.F : num |\n",
    "5. Fare : num | 19. Ticket.4 : num | 33. Cabin.T : num |\n",
    "6. Embarked.S : num | 20. Ticket.F : num | 34. Title.Master : num |\n",
    "7. Embarked.C : num | 21. Ticket.L : num | 35. Title.Rev : num |\n",
    "8. Embarked.Q : num | 22. Ticket.9 : num | 36. Title.Mr : num |\n",
    "9. FamilySize : num | 23. Ticket.6 : num | 37. Title.Miss : num |\n",
    "10. Ticket.A : num | 24. Ticket.5 : num | 38. Title.Dr : num |\n",
    "11. Ticket.P : num | 25. Ticket.8 : num | 39. Title.Mrs : num |\n",
    "12. Ticket.S : num | 26. Cabin.C : num | 40. Title.Military : num |\n",
    "13. Ticket.1 : num | 27. Cabin.E : num |\n",
    "\n",
    "Survived Pclass Sex.male Sex.female Age\n",
    "0 : 549 Min. : -1.565 Min. : -1.355 Min. : -0.737 Min. : -2.781\n",
    "1 : 342 1st Qu. : -0.369 1st Qu. : -1.355 1st Qu. : -0.737 1st Qu. : -0.336\n",
    "Median : 0.827 Median : 0.737 Median : -0.737 Median : -0.113\n",
    "Mean : -0.000 Mean : -0.000 Mean : 0.000 Mean : 0.000\n",
    "2nd Qu. : 0.827 2nd Qu. : 0.737 2nd Qu. : 1.355 2nd Qu. : 0.483\n",
    "Max. : 0.827 Max. : 0.737 Max. : 1.355 Max. : 2.929\n",
    "Fare Embarked.S Embarked.C Embarked.Q FamilySize\n",
    "Min. : -0.648 Min. : -1.632 Min. : -0.482 Min. : -0.303 Min. : -0.561\n",
    "1st Qu. : -0.489 1st Qu. : -1.632 1st Qu. : -0.482 1st Qu. : -0.303 1st Qu. : -0.561\n",
    "Median : -0.357 Median : 0.612 Median : -0.482 Median : -0.303 Median : -0.561\n",
    "Mean : 0.000 Mean : 0.000 Mean : 0.000 Mean : 0.000 Mean : -0.000\n",
    "2nd Qu. : -0.024 2nd Qu. : 0.612 2nd Qu. : -0.482 2nd Qu. : -0.303 2nd Qu. : 0.059\n",
    "Max. : 9.662 Max. : 0.612 Max. : 2.073 Max. : 3.297 Max. : 5.637\n",
    "Ticket.A Ticket.P Ticket.S Ticket.1 Ticket.3\n",
    "Min. : -0.139 Min. : -0.280 Min. : -0.251 Min. : -0.433 Min. : -0.767\n",
    "1st Qu. : -0.139 1st Qu. : -0.280 1st Qu. : -0.251 1st Qu. : -0.433 1st Qu. : -0.767\n",
    "Median : -0.139 Median : -0.280 Median : -0.251 Median : -0.433 Median : -0.767\n",
    "Mean : -0.000 Mean : -0.000 Mean : -0.000 Mean : -0.000 Mean : 0.000\n",
    "2nd Qu. : -0.139 2nd Qu. : -0.280 2nd Qu. : -0.251 2nd Qu. : -0.433 2nd Qu. : 1.303\n",
    "Max. : 7.166 Max. : 3.563 Max. : 3.974 Max. : 2.305 Max. : 1.303\n",
    "Ticket.2 Ticket.C Ticket.7 Ticket.W Ticket.4\n",
    "Min. : -0.508 Min. : -0.261 Min. : -0.101 Min. : -0.101 Min. : -0.106\n",
    "1st Qu. : -0.508 1st Qu. : -0.261 1st Qu. : -0.101 1st Qu. : -0.101 1st Qu. : -0.106\n",
    "Median : -0.508 Median : -0.261 Median : -0.101 Median : -0.101 Median : -0.106\n",
    "Mean : 0.000 Mean : -0.000 Mean : -0.000 Mean : 0.000 Mean : 0.000\n",
    "2nd Qu. : -0.508 2nd Qu. : -0.261 2nd Qu. : -0.101 2nd Qu. : -0.101 2nd Qu. : -0.106\n",
    "Max. : 1.966 Max. : 3.823 Max. : 9.894 Max. : 9.894 Max. : 9.381\n",
    "Ticket.F Ticket.L Ticket.9 Ticket.6 Ticket.5\n",
    "Min. : -0.082 Min. : -0.067 Min. : 0.000 Min. : -0.067 Min. : -0.034\n",
    "1st Qu. : -0.082 1st Qu. : -0.067 1st Qu. : 0.000 1st Qu. : -0.067 1st Qu. : -0.034\n",
    "Median : -0.082 Median : -0.067 Median : 0.000 Median : -0.067 Median : -0.034\n",
    "Mean : -0.000 Mean : -0.000 Mean : 0.000 Mean : -0.000 Mean : -0.000\n",
    "2nd Qu. : -0.082 2nd Qu. : -0.067 2nd Qu. : 0.000 2nd Qu. : -0.067 2nd Qu. : -0.034\n",
    "Max. : 12.138 Max. : 14.883 Max. : 0.000 Max. : 14.883 Max. : 29.816\n",
    "Ticket.8 Cabin.C Cabin.E Cabin.G Cabin.D\n",
    "Min. : -0.047 Min. : -0.379 Min. : -0.615 Min. : -0.301 Min. : -0.499\n",
    "1st Qu. : -0.047 1st Qu. : -0.379 1st Qu. : -0.615 1st Qu. : -0.301 1st Qu. : -0.499\n",
    "Median : -0.047 Median : -0.379 Median : -0.615 Median : -0.301 Median : -0.499\n",
    "Mean : -0.000 Mean : -0.000 Mean : 0.000 Mean : -0.000 Mean : 0.000\n",
    "2nd Qu. : -0.047 2nd Qu. : -0.379 2nd Qu. : 1.623 2nd Qu. : -0.301 2nd Qu. : -0.499\n",
    "Max. : 21.071 Max. : 2.636 Max. : 1.623 Max. : 3.321 Max. : 2.000\n",
    "Cabin.A Cabin.B Cabin.F Cabin.T Title.Master\n",
    "Min. : -0.159 Min. : -0.269 Min. : -0.538 Min. : 0.000 Min. : -0.217\n",
    "1st Qu. : -0.159 1st Qu. : -0.269 1st Qu. : -0.538 1st Qu. : 0.000 1st Qu. : -0.217\n",
    "Median : -0.159 Median : -0.269 Median : -0.538 Median : 0.000 Median : -0.217\n",
    "Mean : 0.000 Mean : -0.000 Mean : 0.000 Mean : 0.000 Mean : 0.000\n",
    "2nd Qu. : -0.159 2nd Qu. : -0.269 2nd Qu. : -0.538 2nd Qu. : 0.000 2nd Qu. : -0.217\n",
    "Max. : 6.281 Max. : 3.719 Max. : 1.858 Max. : 0.000 Max. : 4.610\n",
    "Title.Rev Title.Mr Title.Miss Title.Dr Title.Mrs\n",
    "Min. : -0.082 Min. : -1.183 Min. : -0.510 Min. : -0.089 Min. : -0.409\n",
    "1st Qu. : -0.082 1st Qu. : -1.183 1st Qu. : -0.510 1st Qu. : -0.089 1st Qu. : -0.409\n",
    "Median : -0.082 Median : 0.844 Median : -0.510 Median : -0.089 Median : -0.409\n",
    "Mean : -0.000 Mean : -0.000 Mean : -0.000 Mean : 0.000 Mean : -0.000\n",
    "2nd Qu. : -0.082 2nd Qu. : 0.844 2nd Qu. : -0.510 2nd Qu. : -0.089 2nd Qu. : -0.409\n",
    "Max. : 12.138 Max. : 0.844 Max. : 1.959 Max. : 11.231 Max. : 2.440\n",
    "Title.Military\n",
    "Min. : -0.082\n",
    "1st Qu. : -0.082\n",
    "Median : -0.082\n",
    "Mean : 0.000\n",
    "2nd Qu. : -0.082\n",
    "Max. : 12.138\n",
    "```\n",
    "\n",
    "There is a lot of content. Notice that we have numerical variables for each ticket first letter, title, cabin first letter, etc.\n",
    "\n",
    "## Train a polynomial SVM\n",
    "\n",
    "A linear kernel is a polynomial kernel with degree 1. We let the $$C$$ parameter to the default value which is $$1$$.\n",
    "\n",
    "```java\n",
    "Classifier model = new BinarySMO()\n",
    ".withInputFilters(inputFilters)\n",
    ".withC(0.0001)\n",
    ".withKernel(new PolyKernel(1));\n",
    "model.train(train, \"Survived\");\n",
    "CFit fit = model.fit(test);\n",
    "new Confusion(train.getVar(\"Survived\"), model.fit(train).firstClasses()).printSummary();\n",
    "Csv.instance().withQuotes(false).write(SolidFrame.wrapOf(\n",
    "test.getVar(\"PassengerId\"),\n",
    "fit.firstClasses().withName(\"Survived\")\n",
    "), root + \"svm1-submit.csv\");\n",
    "cv(train, model);\n",
    "```\n",
    "```\n",
    "> Confusion\n",
    "\n",
    "Ac\\Pr | 0 1 | total\n",
    "----- | - - | -----\n",
    "0 | >292 257 | 549\n",
    "1 | 41 >301 | 342\n",
    "----- | - - | -----\n",
    "total | 333 558 | 891\n",
    "\n",
    "\n",
    "Complete cases 891 from 891\n",
    "Acc: 0.6655443 (Accuracy )\n",
    "F1: 0.6621315 (F1 score / F-measure)\n",
    "MCC: 0.4141426 (Matthew correlation coefficient)\n",
    "Pre: 0.8768769 (Precision)\n",
    "Rec: 0.5318761 (Recall)\n",
    "G: 0.6829274 (G-measure)\n",
    "Cross validation 10-fold\n",
    "CV fold: 1, acc: 0.666667, mean: 0.666667, se: NaN\n",
    "CV fold: 2, acc: 0.764045, mean: 0.715356, se: 0.068857\n",
    "CV fold: 3, acc: 0.775281, mean: 0.735331, se: 0.059730\n",
    "CV fold: 4, acc: 0.707865, mean: 0.728464, se: 0.050666\n",
    "CV fold: 5, acc: 0.719101, mean: 0.726592, se: 0.044077\n",
    "CV fold: 6, acc: 0.696629, mean: 0.721598, se: 0.041278\n",
    "CV fold: 7, acc: 0.842697, mean: 0.738898, se: 0.059286\n",
    "CV fold: 8, acc: 0.730337, mean: 0.737828, se: 0.054972\n",
    "CV fold: 9, acc: 0.752809, mean: 0.739492, se: 0.051663\n",
    "CV fold:10, acc: 0.808989, mean: 0.746442, se: 0.053437\n",
    "=================\n",
    "mean: 0.746442, se: 0.053437\n",
    "```\n",
    "\n",
    "The results are not promising. This is better than random but it is not enough for our purpose. There are some explanations for this result. First one could be that if the space would be linear, than the original feature space would be the same as transformed. This means that a classifierModel as random forest would work well if the linear svm would have worked. This might not be true in general, but in this case it looks like a good explanation. We need to be more flexible.\n",
    "\n",
    "To increase the flexibility of the model and to allow features to interact with one another we change the degree of the polynomial kernel. This time we will use `degree=3`. Also, we use $$C=0.0001$$ to allow for some errors. This parameter is the factor of the slack regularization constraints of the SVM optimization problem. The bigger the value the more is the penalty for wrong decisions. If the space would be linear separable than one can theoretically set this value as high as possible. But we know it is not. Also we know that we have plenty of irreducible error. As a consequence, it looks like we should decrease the value of this parameter.\n",
    "\n",
    "```java\n",
    "Classifier model = new BinarySMO()\n",
    ".withInputFilters(inputFilters)\n",
    ".withC(0.0001)\n",
    ".withKernel(new PolyKernel(1));\n",
    "model.train(train, \"Survived\");\n",
    "CFit fit = model.fit(test);\n",
    "new Confusion(train.getVar(\"Survived\"), model.fit(train).firstClasses()).printSummary();\n",
    "Csv.instance().withQuotes(false).write(SolidFrame.wrapOf(\n",
    "test.getVar(\"PassengerId\"),\n",
    "fit.firstClasses().withName(\"Survived\")\n",
    "), root + \"svm1-submit.csv\");\n",
    "cv(train, model);\n",
    "```\n",
    "```\n",
    "> Confusion\n",
    "\n",
    "Ac\\Pr | 0 1 | total\n",
    "----- | - - | -----\n",
    "0 | >472 77 | 549\n",
    "1 | 48 >294 | 342\n",
    "----- | - - | -----\n",
    "total | 520 371 | 891\n",
    "\n",
    "\n",
    "Complete cases 891 from 891\n",
    "Acc: 0.8597082 (Accuracy )\n",
    "F1: 0.8830683 (F1 score / F-measure)\n",
    "MCC: 0.7097044 (Matthew correlation coefficient)\n",
    "Pre: 0.9076923 (Precision)\n",
    "Rec: 0.859745 (Recall)\n",
    "G: 0.8833934 (G-measure)\n",
    "Cross validation 10-fold\n",
    "CV fold: 1, acc: 0.822222, mean: 0.822222, se: NaN\n",
    "CV fold: 2, acc: 0.786517, mean: 0.804370, se: 0.025248\n",
    "CV fold: 3, acc: 0.853933, mean: 0.820891, se: 0.033728\n",
    "CV fold: 4, acc: 0.808989, mean: 0.817915, se: 0.028174\n",
    "CV fold: 5, acc: 0.797753, mean: 0.813883, se: 0.026012\n",
    "CV fold: 6, acc: 0.786517, mean: 0.809322, se: 0.025809\n",
    "CV fold: 7, acc: 0.831461, mean: 0.812484, se: 0.025002\n",
    "CV fold: 8, acc: 0.876404, mean: 0.820474, se: 0.032350\n",
    "CV fold: 9, acc: 0.764045, mean: 0.814204, se: 0.035631\n",
    "CV fold:10, acc: 0.820225, mean: 0.814806, se: 0.033647\n",
    "=================\n",
    "mean: 0.814806, se: 0.033647\n",
    "```\n",
    "\n",
    "This time the results are promising. We achieved a training error which is not close to zero and the cross validation errors are close to our desired results. We definitely should try this classifierModel.\n",
    "\n",
    "![SVM1](images/titanic-svm1-submit.png)\n",
    "\n",
    "We have a better score also on public leader board. Which is very fine. Usually in this competition a score in $$0.75-0.78$$ is fine and one in $$0.78-0.81$$ is excellent.\n",
    "\n",
    "## Tuning manually the SVM\n",
    "\n",
    "We can work more on SVMs. One thing which deserves a try is the radial basis kernel. This is similar with working in an infinite dimensional space! We tried some `RBFKernel` approaches, but much better results gave the `CauchyKernel`. The `CauchyKernel` works in a similar way like a RBF kernel. The difference which sometimes is important is that it is a distribution with tails fatter than Gaussian distribution. This produces an effect of long distance influence. This is reasonable to use in this problem because we know we have noise. We can think that a kernel which acts on wider ranges is better if it is combined with a small value for $$C$$.\n",
    "\n",
    "After some manual tuning we arrived at the following classifierModel.\n",
    "\n",
    "```java\n",
    "Classifier model = new BinarySMO()\n",
    ".withInputFilters(inputFilters)\n",
    ".withC(1)\n",
    ".withTol(1e-10)\n",
    ".withKernel(new CauchyKernel(25));\n",
    "```\n",
    "```\n",
    "> Confusion\n",
    "\n",
    "Ac\\Pr | 0 1 | total\n",
    "----- | - - | -----\n",
    "0 | >520 29 | 549\n",
    "1 | 104 >238 | 342\n",
    "----- | - - | -----\n",
    "total | 624 267 | 891\n",
    "\n",
    "\n",
    "Complete cases 891 from 891\n",
    "Acc: 0.8507295 (Accuracy )\n",
    "F1: 0.8866155 (F1 score / F-measure)\n",
    "MCC: 0.6826819 (Matthew correlation coefficient)\n",
    "Pre: 0.8333333 (Precision)\n",
    "Rec: 0.9471767 (Recall)\n",
    "G: 0.8884334 (G-measure)\n",
    "Cross validation 10-fold\n",
    "CV fold: 1, acc: 0.811111, mean: 0.811111, se: NaN\n",
    "CV fold: 2, acc: 0.786517, mean: 0.798814, se: 0.017391\n",
    "CV fold: 3, acc: 0.865169, mean: 0.820932, se: 0.040235\n",
    "CV fold: 4, acc: 0.797753, mean: 0.815137, se: 0.034836\n",
    "CV fold: 5, acc: 0.831461, mean: 0.818402, se: 0.031040\n",
    "CV fold: 6, acc: 0.842697, mean: 0.822451, se: 0.029481\n",
    "CV fold: 7, acc: 0.820225, mean: 0.822133, se: 0.026926\n",
    "CV fold: 8, acc: 0.820225, mean: 0.821895, se: 0.024937\n",
    "CV fold: 9, acc: 0.797753, mean: 0.819212, se: 0.024676\n",
    "CV fold:10, acc: 0.831461, mean: 0.820437, se: 0.023585\n",
    "=================\n",
    "mean: 0.820437, se: 0.023585\n",
    "```\n",
    "\n",
    "This classifierModel has similar results, but there are reasons to believe that it is slightly better than previous. The training error smaller. But we know that training error is not a good estimator. The 10 fold cv is greater. This is a good sign. A better interpretation would be that the gap between those two has shrunken and this is a good thing. A new submit on kaggle follows.\n",
    "\n",
    "![SVM2](images/titanic-svm2-submit.png)\n",
    "\n",
    "Well we are really, really close to our psychological milestone of $$0.8$$. Perhaps some tuning will give more results. This is true in general. However, next section provides you with a better approach which usually provides some gain in accuracy: stacking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Stacking classifiers\n",
    "\n",
    "Using random forests or SVMs did not provided us with a result over $$0.8$$. We have two very different types of models which performed well. For the sole purpose of prediction we use a nice ensemble technique which often provides good prediction performance gain. This technique is called stacking.\n",
    "\n",
    "The idea behind stacking is that one can explore the space of the solutions with different approaches. Each approach (or statistical model) is basically an interpretation of the solution. But often times a proper interpretation is really hard to find. Each interpretation of the solution can have good points and weak points. The idea is to blend those interpretations into a single one in a way that we try somehow to keep what is string from each individual classifierModel.\n",
    "\n",
    "A stacking classifierModel take some base learners and train them on training data. The results of the base learners are used as input for a stacking learner. This stacking learner is trained on the output of base learners and target variable and is finally used for prediction.\n",
    "\n",
    "```java\n",
    "Classifier model = new CStacking()\n",
    ".withLearners(\n",
    "new BinarySMO()\n",
    ".withInputFilters(inputFilters)\n",
    ".withC(1)\n",
    ".withTol(1e-10)\n",
    ".withKernel(new CauchyKernel(20)),\n",
    "CForest.newRF()\n",
    ".withInputFilters(inputFilters)\n",
    ".withMCols(4)\n",
    ".withBootstrap(0.07)\n",
    ".withClassifier(CTree.newCART()\n",
    ".withFunction(CTreePurityFunction.GainRatio)\n",
    ".withMinGain(0.001))\n",
    ".withRuns(200)\n",
    ").withStacker(CForest.newRF().withBootstrap(0.3)\n",
    ".withRuns(200)\n",
    ".withClassifier(CTree.newCART()\n",
    ".withFunction(CTreePurityFunction.GainRatio)\n",
    ".withMinGain(0.05)));\n",
    "```\n",
    "\n",
    "Usually one uses a binary logistic regression model but it provided weak results. What looked much better is another random forrest classifierModel. However the stacking model uses a big value for minimum gain parameter because we want to act as an draft average over the results.\n",
    "\n",
    "```\n",
    "> Confusion\n",
    "\n",
    "Ac\\Pr | 0 1 | total\n",
    "----- | - - | -----\n",
    "0 | >522 27 | 549\n",
    "1 | 87 >255 | 342\n",
    "----- | - - | -----\n",
    "total | 609 282 | 891\n",
    "\n",
    "\n",
    "Complete cases 891 from 891\n",
    "Acc: 0.8720539 (Accuracy )\n",
    "F1: 0.9015544 (F1 score / F-measure)\n",
    "MCC: 0.7281918 (Matthew correlation coefficient)\n",
    "Pre: 0.8571429 (Precision)\n",
    "Rec: 0.9508197 (Recall)\n",
    "G: 0.902767 (G-measure)\n",
    "Cross validation 10-fold\n",
    "CV fold: 1, acc: 0.888889, mean: 0.888889, se: NaN\n",
    "CV fold: 2, acc: 0.786517, mean: 0.837703, se: 0.072388\n",
    "CV fold: 3, acc: 0.842697, mean: 0.839367, se: 0.051267\n",
    "CV fold: 4, acc: 0.820225, mean: 0.834582, se: 0.042940\n",
    "CV fold: 5, acc: 0.808989, mean: 0.829463, se: 0.038908\n",
    "CV fold: 6, acc: 0.853933, mean: 0.833541, se: 0.036206\n",
    "CV fold: 7, acc: 0.786517, mean: 0.826824, se: 0.037527\n",
    "CV fold: 8, acc: 0.898876, mean: 0.835830, se: 0.043082\n",
    "CV fold: 9, acc: 0.831461, mean: 0.835345, se: 0.040326\n",
    "CV fold:10, acc: 0.775281, mean: 0.829338, se: 0.042500\n",
    "=================\n",
    "mean: 0.829338, se: 0.042500\n",
    "```\n",
    "Again, the results are promising. The space between training error and cross validation error is smaller and our expectations grows.\n",
    "\n",
    "![Stacking with a random forest](images/titanic-stacking1-submit.png)\n",
    "\n",
    "Finally our target performance was achieved!!\n",
    "\n",
    "**Note:**\n",
    "\n",
    "The general advice in real life is to not fight for each piece of performance measure. It really depends on the question one wants to answer. Often measures like ROC or partial ROC are much better than error frequency. We fixed this milestone because we know it is possible and because it looks like a psychological difficulty. (that $$0.799$$ is outrageous)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".jshell",
   "mimetype": "text/x-java-source",
   "name": "Java",
   "pygments_lexer": "java",
   "version": "16.0.1+9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
